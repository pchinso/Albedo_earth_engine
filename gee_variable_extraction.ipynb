{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbcwRI4e3cAc"
      },
      "source": [
        "# Google Earth Engine Variable Extraction Notebook\n",
        "\n",
        "This notebook provides functions to extract various environmental variables from Google Earth Engine (GEE) datasets.\n",
        "The primary goals are:\n",
        "- Define functions to retrieve specific variables based on user-defined parameters (region, time period, frequency).\n",
        "- For time-series data, aggregate it to hourly, daily, monthly, or yearly means.\n",
        "- Save the extracted data for each variable into a separate CSV file.\n",
        "\n",
        "**Instructions:**\n",
        "1. Run the GEE Authentication and Initialization cell first. You will need to authenticate with a Google account that has GEE access.\n",
        "2. Define your region of interest (AOI) as a GeoJSON-like dictionary.\n",
        "3. Call the specific extraction functions for the variables you need, providing the AOI, date range, frequency, and output directory."
      ],
      "id": "JbcwRI4e3cAc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this once per session\n",
        "# Update folium to latest version\n",
        "\n",
        "!sudo apt-get install wkhtmltopdf\n",
        "!pip install folium html2image\n",
        "!pip install --upgrade folium\n",
        "!pip install playwright\n",
        "!playwright install"
      ],
      "metadata": {
        "id": "vez1d0dvFZDD",
        "outputId": "f6cb81a7-07af-4c24-e952-7b0ac3559493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vez1d0dvFZDD",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "wkhtmltopdf is already the newest version (0.12.6-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 90 not upgraded.\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
            "Requirement already satisfied: html2image in /usr/local/lib/python3.11/dist-packages (2.0.7)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from folium) (0.8.1)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from folium) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from folium) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from folium) (2.32.3)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.11/dist-packages (from folium) (2025.4.0)\n",
            "Requirement already satisfied: websocket-client~=1.0 in /usr/local/lib/python3.11/dist-packages (from html2image) (1.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.9->folium) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (2025.4.26)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from folium) (0.8.1)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from folium) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from folium) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from folium) (2.32.3)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.11/dist-packages (from folium) (2025.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.9->folium) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SHiLFHIt3cAd"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import os\n",
        "import csv\n",
        "import math # Ensure math is imported for potential calculations like wind\n",
        "import calendar"
      ],
      "id": "SHiLFHIt3cAd"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0_VI4WPE3cAd",
        "outputId": "c41be42f-e8e8-4999-c0d8-2a21640c1d69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GEE Initialized successfully.\n",
            "GEE is initialized and accessible.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Trigger the authentication flow.\n",
        "# This will print a URL, open it, authorize, and copy the code back into the input box.\n",
        "try:\n",
        "    ee.Authenticate()\n",
        "except Exception as e:\n",
        "    print(f\"Authentication failed or already authenticated: {e}\")\n",
        "    # For automated environments or if auth was done in a previous session,\n",
        "    # this might raise an error if not needed, so we can often proceed.\n",
        "\n",
        "# Initialize the library.\n",
        "# Replace 'YOUR_GEE_PROJECT' with your actual GEE project ID if you have one,\n",
        "# otherwise, GEE often can use a default cloud project associated with your account.\n",
        "try:\n",
        "    ee.Initialize(project='gen-lang-client-0253961861')\n",
        "    print(\"GEE Initialized successfully.\")\n",
        "except Exception as e:\n",
        "    try:\n",
        "        # Fallback if project-specific initialization fails\n",
        "        ee.Initialize()\n",
        "        print(\"GEE Initialized successfully (default project).\")\n",
        "    except Exception as e_init:\n",
        "        print(f\"GEE Initialization failed: {e_init}\")\n",
        "        print(\"Please ensure you have authenticated and have a GEE-enabled project.\")\n",
        "\n",
        "# Define a helper to check GEE initialization status\n",
        "def check_gee_initialized():\n",
        "    try:\n",
        "        ee.ImageCollection('MODIS/061/MCD43A3').limit(1).size().getInfo()\n",
        "        print(\"GEE is initialized and accessible.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"GEE not properly initialized or accessible: {e}\")\n",
        "        return False\n",
        "\n",
        "check_gee_initialized()"
      ],
      "id": "0_VI4WPE3cAd"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XHlRJn9Y3cAd"
      },
      "outputs": [],
      "source": [
        "# Generic Time Series Extraction Function\n",
        "def extract_gee_time_series(\n",
        "    variable_name: str,\n",
        "    region_geojson: dict, # GeoJSON dictionary for the region\n",
        "    start_date_str: str,\n",
        "    end_date_str: str,\n",
        "    frequency: str,  # 'hourly', 'daily', 'monthly', 'yearly'\n",
        "    gee_dataset_id: str,\n",
        "    gee_band_name: str, # Can be a list for multi-band calculations (e.g. wind)\n",
        "    scale: int,\n",
        "    output_dir: str,\n",
        "    gee_project: str = None, # Optional: GEE project for initialization if needed\n",
        "    reducer: ee.Reducer = ee.Reducer.mean(),\n",
        "    data_scaling_factor: float = None,\n",
        "    data_offset_factor: float = None,\n",
        "    post_process_function: callable = None, # Function to apply to the reduced value or dictionary of values\n",
        "    nan_value = None # Value to use if GEE returns None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Extracts time-series data from Google Earth Engine for a specified variable and frequency,\n",
        "    saves it to a CSV file, and returns the path to the CSV.\n",
        "\n",
        "    Parameters:\n",
        "    - variable_name: Name of the variable (used for CSV filename).\n",
        "    - region_geojson: GeoJSON dictionary defining the region of interest.\n",
        "    - start_date_str: Start date in 'YYYY-MM-DD' format.\n",
        "    - end_date_str: End date in 'YYYY-MM-DD' format.\n",
        "    - frequency: Aggregation frequency ('hourly', 'daily', 'monthly', 'yearly').\n",
        "    - gee_dataset_id: Earth Engine ImageCollection ID.\n",
        "    - gee_band_name: Name of the band(s) to extract. If a list, post_process_function must handle it.\n",
        "    - scale: Spatial resolution in meters for reduction.\n",
        "    - output_dir: Directory to save the output CSV file.\n",
        "    - gee_project: Optional GEE project ID for ee.Initialize().\n",
        "    - reducer: Earth Engine reducer to apply (default: ee.Reducer.mean()).\n",
        "    - data_scaling_factor: Optional factor to multiply the band data by.\n",
        "    - data_offset_factor: Optional offset to add to the band data.\n",
        "    - post_process_function: Optional function to apply to the raw reduced value(s).\n",
        "                             It should accept a dictionary of band values if multiple bands are processed,\n",
        "                             or a single value if one band is processed. It should return a dictionary\n",
        "                             of processed values or a single processed value.\n",
        "    - nan_value: Value to fill in if GEE returns no data for a period.\n",
        "\n",
        "    Returns:\n",
        "    - Path to the generated CSV file.\n",
        "    \"\"\"\n",
        "    if gee_project:\n",
        "        try:\n",
        "            ee.Initialize(project=gee_project)\n",
        "        except Exception:\n",
        "            ee.Initialize() # Fallback\n",
        "\n",
        "    ee_region = ee.Geometry(region_geojson)\n",
        "\n",
        "    start_date = datetime.datetime.strptime(start_date_str, '%Y-%m-%d')\n",
        "    end_date = datetime.datetime.strptime(end_date_str, '%Y-%m-%d')\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    date_periods = []\n",
        "    current_date = start_date\n",
        "\n",
        "    if frequency == 'hourly':\n",
        "        delta = relativedelta(hours=1)\n",
        "        date_format_label = '%Y-%m-%d %H:%M:%S'\n",
        "        while current_date <= end_date:\n",
        "            period_end_date = current_date + delta - relativedelta(seconds=1) # End of the hour\n",
        "            date_periods.append({\n",
        "                \"start\": current_date.strftime('%Y-%m-%dT%H:%M:%S'),\n",
        "                \"end\": period_end_date.strftime('%Y-%m-%dT%H:%M:%S'),\n",
        "                \"label\": current_date.strftime(date_format_label)\n",
        "            })\n",
        "            current_date += delta\n",
        "    elif frequency == 'daily':\n",
        "        delta = relativedelta(days=1)\n",
        "        date_format_label = '%Y-%m-%d'\n",
        "        while current_date <= end_date:\n",
        "            date_periods.append({\n",
        "                \"start\": current_date.strftime('%Y-%m-%d'),\n",
        "                \"end\": (current_date + delta - relativedelta(days=1)).strftime('%Y-%m-%d'), # inclusive end\n",
        "                \"label\": current_date.strftime(date_format_label)\n",
        "            })\n",
        "            current_date += delta\n",
        "    elif frequency == 'monthly':\n",
        "        delta = relativedelta(months=1)\n",
        "        date_format_label = '%Y-%m'\n",
        "        while current_date <= end_date:\n",
        "            month_start = current_date.replace(day=1)\n",
        "            month_end = month_start + delta - relativedelta(days=1)\n",
        "            date_periods.append({\n",
        "                \"start\": month_start.strftime('%Y-%m-%d'),\n",
        "                \"end\": month_end.strftime('%Y-%m-%d'),\n",
        "                \"label\": month_start.strftime(date_format_label)\n",
        "            })\n",
        "            current_date += delta\n",
        "    elif frequency == 'yearly':\n",
        "        delta = relativedelta(years=1)\n",
        "        date_format_label = '%Y'\n",
        "        while current_date <= end_date:\n",
        "            year_start = current_date.replace(month=1, day=1)\n",
        "            year_end = year_start + delta - relativedelta(days=1)\n",
        "            date_periods.append({\n",
        "                \"start\": year_start.strftime('%Y-%m-%d'),\n",
        "                \"end\": year_end.strftime('%Y-%m-%d'),\n",
        "                \"label\": year_start.strftime(date_format_label)\n",
        "            })\n",
        "            current_date += delta\n",
        "    else:\n",
        "        raise ValueError(\"Invalid frequency. Choose from 'hourly', 'daily', 'monthly', 'yearly'.\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for period in date_periods:\n",
        "        try:\n",
        "            collection = ee.ImageCollection(gee_dataset_id) \\\n",
        "                           .filterDate(ee.Date(period[\"start\"]), ee.Date(period[\"end\"]).advance(1, 'day')) # GEE end date is exclusive\n",
        "\n",
        "            if isinstance(gee_band_name, list): # For multi-band variables like wind\n",
        "                selected_bands_collection = collection.select(gee_band_name)\n",
        "            else: # Single band\n",
        "                selected_bands_collection = collection.select([gee_band_name])\n",
        "\n",
        "            # Check if collection is empty for the period\n",
        "            if selected_bands_collection.size().getInfo() == 0:\n",
        "                print(f\"No images found for {variable_name} in period {period['label']} for dataset {gee_dataset_id}\")\n",
        "                reduced_value = nan_value\n",
        "                if isinstance(gee_band_name, list) and nan_value is not None:\n",
        "                     # If multiple bands expected, fill with nan_value for each\n",
        "                    reduced_value = {band: nan_value for band in gee_band_name}\n",
        "\n",
        "                if post_process_function and reduced_value is not None : # nan_value can be processed if needed\n",
        "                     processed_value = post_process_function(reduced_value)\n",
        "                else:\n",
        "                     processed_value = reduced_value\n",
        "\n",
        "                # Ensure processed_value is a dictionary for DataFrame creation\n",
        "                if not isinstance(processed_value, dict) and isinstance(gee_band_name, list):\n",
        "                    # if single value came from post_process for multiple bands, try to map it\n",
        "                    # this might need adjustment based on post_process_function's behavior\n",
        "                    processed_value = {f\"{variable_name}_{b}\" if len(gee_band_name) > 1 else variable_name : processed_value for b in gee_band_name}\n",
        "                elif not isinstance(processed_value, dict):\n",
        "                    processed_value = {variable_name: processed_value}\n",
        "\n",
        "            else:\n",
        "                image_for_period = selected_bands_collection.mean() # Temporal aggregation\n",
        "\n",
        "                if data_scaling_factor is not None:\n",
        "                    image_for_period = image_for_period.multiply(data_scaling_factor)\n",
        "                if data_offset_factor is not None:\n",
        "                    image_for_period = image_for_period.add(data_offset_factor)\n",
        "\n",
        "                # Perform reduction\n",
        "                reduction = image_for_period.reduceRegion(\n",
        "                    reducer=reducer,\n",
        "                    geometry=ee_region,\n",
        "                    scale=scale,\n",
        "                    maxPixels=1e10,\n",
        "                    bestEffort=True, # Added bestEffort\n",
        "                    tileScale=0.1 # Added tileScale\n",
        "                )\n",
        "\n",
        "                raw_reduced_value = {}\n",
        "                if isinstance(gee_band_name, list):\n",
        "                    for band in gee_band_name:\n",
        "                        raw_reduced_value[band] = reduction.get(band).getInfo()\n",
        "                else:\n",
        "                    raw_reduced_value = reduction.get(gee_band_name).getInfo()\n",
        "\n",
        "                if post_process_function:\n",
        "                    processed_value = post_process_function(raw_reduced_value)\n",
        "                else:\n",
        "                    processed_value = raw_reduced_value\n",
        "\n",
        "            # Structure for DataFrame\n",
        "            current_row = {'timestamp': period['label']}\n",
        "            if isinstance(processed_value, dict):\n",
        "                current_row.update(processed_value)\n",
        "            else: # Single value result\n",
        "                current_row[variable_name] = processed_value\n",
        "            results.append(current_row)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing period {period['label']} for {variable_name}: {e}\")\n",
        "            # Add a row with nan_value or error indication\n",
        "            error_entry = {'timestamp': period['label']}\n",
        "            if isinstance(gee_band_name, list):\n",
        "                for band in gee_band_name:\n",
        "                    error_entry[f\"{variable_name}_{band}\"] = nan_value  # Or some error string\n",
        "            else:\n",
        "                error_entry[variable_name] = nan_value # Or some error string\n",
        "            results.append(error_entry)\n",
        "            continue # Continue to next period\n",
        "\n",
        "    if not results:\n",
        "        print(f\"No data extracted for {variable_name} in the given period.\")\n",
        "        return None\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    # Ensure timestamp is the first column\n",
        "    cols = ['timestamp'] + [col for col in df.columns if col != 'timestamp']\n",
        "    df = df[cols]\n",
        "\n",
        "    # Sanitize filename\n",
        "    safe_start_date = start_date_str.replace('-', '')\n",
        "    safe_end_date = end_date_str.replace('-', '')\n",
        "    csv_filename = f\"{variable_name}_{frequency}_{safe_start_date}_{safe_end_date}.csv\"\n",
        "    csv_path = os.path.join(output_dir, csv_filename)\n",
        "    df.to_csv(csv_path, index=False, na_rep=str(nan_value) if nan_value is not None else 'NaN') # Use nan_value for na_rep\n",
        "\n",
        "    print(f\"Successfully saved data for {variable_name} to {csv_path}\")\n",
        "    return csv_path\n",
        "\n",
        "# --- Helper function for MODIS LST to Celsius ---\n",
        "def convert_modis_lst_to_celsius(lst_value):\n",
        "    \"\"\"Converts MODIS LST (scaled Kelvin) to Celsius.\"\"\"\n",
        "    if lst_value is None:\n",
        "        return None\n",
        "    return (lst_value * 0.02) - 273.15\n",
        "\n",
        "# --- Specific Variable Extraction Functions ---\n",
        "\n",
        "# Albedo\n",
        "def extract_albedo_bsa(region_geojson, start_date_str, end_date_str, frequency, output_dir, scale=500):\n",
        "    return extract_gee_time_series(\n",
        "        variable_name=\"Albedo_BSA\",\n",
        "        region_geojson=region_geojson,\n",
        "        start_date_str=start_date_str,\n",
        "        end_date_str=end_date_str,\n",
        "        frequency=frequency,\n",
        "        gee_dataset_id='MODIS/061/MCD43A3', # As per reference notebook\n",
        "        gee_band_name='Albedo_BSA_shortwave', # As per reference notebook\n",
        "        scale=scale,\n",
        "        output_dir=output_dir,\n",
        "        data_scaling_factor=0.001, # MODIS albedo scaling\n",
        "        nan_value=np.nan # Use numpy's NaN for missing values\n",
        "    )\n",
        "\n",
        "def extract_albedo_wsa(region_geojson, start_date_str, end_date_str, frequency, output_dir, scale=500):\n",
        "    return extract_gee_time_series(\n",
        "        variable_name=\"Albedo_WSA\",\n",
        "        region_geojson=region_geojson,\n",
        "        start_date_str=start_date_str,\n",
        "        end_date_str=end_date_str,\n",
        "        frequency=frequency,\n",
        "        gee_dataset_id='MODIS/061/MCD43A3', # As per reference notebook\n",
        "        gee_band_name='Albedo_WSA_shortwave', # As per reference notebook\n",
        "        scale=scale,\n",
        "        output_dir=output_dir,\n",
        "        data_scaling_factor=0.001, # MODIS albedo scaling\n",
        "        nan_value=np.nan\n",
        "    )\n",
        "\n",
        "# Solar Radiation (using LST Day as proxy, similar to reference notebook)\n",
        "# Note: This is an approximation. A direct solar radiation dataset would be better if available and suitable.\n",
        "def post_process_solar_radiation(lst_day_value):\n",
        "    \"\"\"Approximates solar radiation from MODIS LST Day value.\"\"\"\n",
        "    if lst_day_value is None:\n",
        "        return None\n",
        "    # If input is a dict (from multi-band processing in generic func),\n",
        "    # extract the value. This handles cases where post_process_function\n",
        "    # might receive a dict even for single-band selection if the generic\n",
        "    # function's internal logic changes or if it's called directly with a dict.\n",
        "    if isinstance(lst_day_value, dict):\n",
        "        val = lst_day_value.get('LST_Day_1km', None)\n",
        "        if val is None: return None\n",
        "    else:\n",
        "        val = lst_day_value\n",
        "\n",
        "    temp_kelvin = val * 0.02 # MODIS LST scaling to Kelvin\n",
        "    stefan_boltzmann = 5.67e-8  # W/(m^2 K^4)\n",
        "    # This is a simplified Stefan-Boltzmann law application, assuming emissivity = 1\n",
        "    # The reference notebook had this calculation. True GHI/DNI would come from datasets like ERA5 or specific solar datasets.\n",
        "    return stefan_boltzmann * (temp_kelvin ** 4)\n",
        "\n",
        "def extract_radiacion_solar(region_geojson, start_date_str, end_date_str, frequency, output_dir, scale=1000):\n",
        "    return extract_gee_time_series(\n",
        "        variable_name=\"Radiacion_Solar_Approximated_from_LST\",\n",
        "        region_geojson=region_geojson,\n",
        "        start_date_str=start_date_str,\n",
        "        end_date_str=end_date_str,\n",
        "        frequency=frequency,\n",
        "        gee_dataset_id='MODIS/061/MOD11A1', # Using LST dataset as per reference\n",
        "        gee_band_name='LST_Day_1km',       # Using LST Day band\n",
        "        scale=scale,\n",
        "        output_dir=output_dir,\n",
        "        post_process_function=post_process_solar_radiation,\n",
        "        nan_value=np.nan\n",
        "    )\n",
        "\n",
        "# Temperature\n",
        "def extract_temperatura_dia(region_geojson, start_date_str, end_date_str, frequency, output_dir, scale=1000):\n",
        "    return extract_gee_time_series(\n",
        "        variable_name=\"Temperatura_Dia_Celsius\",\n",
        "        region_geojson=region_geojson,\n",
        "        start_date_str=start_date_str,\n",
        "        end_date_str=end_date_str,\n",
        "        frequency=frequency,\n",
        "        gee_dataset_id='MODIS/061/MOD11A1', # As per reference notebook\n",
        "        gee_band_name='LST_Day_1km',\n",
        "        scale=scale,\n",
        "        output_dir=output_dir,\n",
        "        post_process_function=lambda x: convert_modis_lst_to_celsius(x.get('LST_Day_1km') if isinstance(x, dict) else x) if x is not None else None, # LST to Celsius\n",
        "        nan_value=np.nan\n",
        "    )\n",
        "\n",
        "def extract_temperatura_noche(region_geojson, start_date_str, end_date_str, frequency, output_dir, scale=1000):\n",
        "    return extract_gee_time_series(\n",
        "        variable_name=\"Temperatura_Noche_Celsius\",\n",
        "        region_geojson=region_geojson,\n",
        "        start_date_str=start_date_str,\n",
        "        end_date_str=end_date_str,\n",
        "        frequency=frequency,\n",
        "        gee_dataset_id='MODIS/061/MOD11A1', # As per reference notebook\n",
        "        gee_band_name='LST_Night_1km',\n",
        "        scale=scale,\n",
        "        output_dir=output_dir,\n",
        "        post_process_function=lambda x: convert_modis_lst_to_celsius(x.get('LST_Night_1km') if isinstance(x, dict) else x) if x is not None else None, # LST to Celsius\n",
        "        nan_value=np.nan\n",
        "    )"
      ],
      "id": "XHlRJn9Y3cAd"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "kd8QV5hk3cAe"
      },
      "outputs": [],
      "source": [
        "# --- Wind Speed and Direction ---\n",
        "def post_process_wind_data(wind_components):\n",
        "    \"\"\"Calculates wind speed and direction from u and v components.\"\"\"\n",
        "    u = wind_components.get('u_component_of_wind_10m')\n",
        "    v = wind_components.get('v_component_of_wind_10m')\n",
        "\n",
        "    if u is None or v is None:\n",
        "        return {'Viento_Velocidad': np.nan, 'Viento_Direccion': np.nan}\n",
        "\n",
        "    speed = math.sqrt(u**2 + v**2)\n",
        "    # Wind direction: meteorological convention (degrees from North, clockwise)\n",
        "    # atan2(u,v) gives angle w.r.t positive y-axis (North). Convert to degrees.\n",
        "    # Then adjust to be 0-360.\n",
        "    # direction_rad = math.atan2(u, v) # u is eastward, v is northward\n",
        "    # direction_deg = math.degrees(direction_rad)\n",
        "    # direction_met = (270 - direction_deg) % 360 # As in reference notebook\n",
        "    # A common formula for meteorological wind direction from u,v components:\n",
        "    direction_met = (180 / math.pi) * math.atan2(-u, -v) + 180\n",
        "    direction_met = direction_met % 360 # Ensure it's within 0-360\n",
        "\n",
        "    return {'Viento_Velocidad': speed, 'Viento_Direccion': direction_met}\n",
        "\n",
        "def extract_viento(region_geojson, start_date_str, end_date_str, frequency, output_dir, scale=10000):\n",
        "    # Note: ERA5 Land is hourly. If 'daily', 'monthly', 'yearly' frequency is requested,\n",
        "    # the generic function will average the hourly u/v components first, then calculate speed/direction.\n",
        "    return extract_gee_time_series(\n",
        "        variable_name=\"Viento\", # Base name, will be expanded by post_process_wind_data keys\n",
        "        region_geojson=region_geojson,\n",
        "        start_date_str=start_date_str,\n",
        "        end_date_str=end_date_str,\n",
        "        frequency=frequency,\n",
        "        gee_dataset_id='ECMWF/ERA5_LAND/HOURLY', # As per reference notebook\n",
        "        gee_band_name=['u_component_of_wind_10m', 'v_component_of_wind_10m'],\n",
        "        scale=scale,\n",
        "        output_dir=output_dir,\n",
        "        post_process_function=post_process_wind_data,\n",
        "        nan_value=np.nan\n",
        "    )\n",
        "\n",
        "# --- Topography (Elevation, Slope, Aspect) ---\n",
        "# These are static, so they don't depend on date range or frequency in the same way.\n",
        "# The function will calculate mean values over the region for a single point in time (the SRTM image is static).\n",
        "def extract_topography(region_geojson, output_dir, scale=30, variable_name_prefix=\"Topografia_\"):\n",
        "    \"\"\"\n",
        "    Extracts mean Elevation, Slope, and Aspect for a region and saves to a CSV.\n",
        "    Output CSV will have one row with columns: 'Elevation', 'Slope', 'Aspect'.\n",
        "    \"\"\"\n",
        "    ee_region = ee.Geometry(region_geojson)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        srtm = ee.Image('USGS/SRTMGL1_003') # SRTM is a single image\n",
        "        elevation = srtm.select('elevation')\n",
        "        slope = ee.Terrain.slope(elevation)\n",
        "        aspect = ee.Terrain.aspect(elevation)\n",
        "\n",
        "        topography_image = ee.Image.cat([elevation, slope, aspect]).rename(['Elevacion', 'Pendiente', 'Aspecto'])\n",
        "\n",
        "        reduction = topography_image.reduceRegion(\n",
        "            reducer=ee.Reducer.mean(),\n",
        "            geometry=ee_region,\n",
        "            scale=scale,\n",
        "            maxPixels=1e10,\n",
        "            bestEffort=True,\n",
        "            tileScale=0.1\n",
        "        )\n",
        "\n",
        "        # GetInfo once\n",
        "        reduced_data = reduction.getInfo()\n",
        "\n",
        "        # Handle cases where a key might be missing (though unlikely for these specific bands from SRTM)\n",
        "        data_for_df = {\n",
        "            'Elevacion': reduced_data.get('Elevacion', np.nan),\n",
        "            'Pendiente': reduced_data.get('Pendiente', np.nan),\n",
        "            'Aspecto': reduced_data.get('Aspecto', np.nan)\n",
        "        }\n",
        "\n",
        "        df = pd.DataFrame([data_for_df])\n",
        "\n",
        "        # Define a simple, non-temporal filename\n",
        "        csv_filename = f\"{variable_name_prefix}mean_values.csv\"\n",
        "        csv_path = os.path.join(output_dir, csv_filename)\n",
        "        df.to_csv(csv_path, index=False, na_rep=str(np.nan))\n",
        "\n",
        "        print(f\"Successfully saved topography data to {csv_path}\")\n",
        "        return csv_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting topography data: {e}\")\n",
        "        # Create a CSV with NaNs if there's an error\n",
        "        df = pd.DataFrame([{'Elevacion': np.nan, 'Pendiente': np.nan, 'Aspecto': np.nan}])\n",
        "        csv_filename = f\"{variable_name_prefix}mean_values_error.csv\"\n",
        "        csv_path = os.path.join(output_dir, csv_filename)\n",
        "        df.to_csv(csv_path, index=False, na_rep=str(np.nan))\n",
        "        return csv_path\n",
        "\n",
        "\n",
        "# --- Land Cover (Principal Type and Percentage) ---\n",
        "# This is typically analyzed for a specific year or period, not as a continuous time series like temperature.\n",
        "# The function will find the dominant land cover type and its percentage for a given year.\n",
        "def extract_land_cover(region_geojson, year_str, output_dir, scale=500, variable_name_prefix=\"Cobertura_\"):\n",
        "    \"\"\"\n",
        "    Extracts the principal land cover type and its percentage for a region and year.\n",
        "    Saves to a CSV with columns: 'Year', 'Cobertura_Terrestre_Principal', 'Cobertura_Terrestre_Porcentaje'.\n",
        "    Uses MODIS MCD12Q1 dataset.\n",
        "    \"\"\"\n",
        "    ee_region = ee.Geometry(region_geojson)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # MODIS Land Cover Type 1 (IGBP classification)\n",
        "        # MCD12Q1 provides yearly data. Filter for the specific year.\n",
        "        start_date = f\"{year_str}-01-01\"\n",
        "        end_date = f\"{year_str}-12-31\" # End of the year\n",
        "\n",
        "        land_cover_collection = ee.ImageCollection('MODIS/061/MCD12Q1') \\\n",
        "                                  .filterDate(start_date, end_date) \\\n",
        "                                  .select('LC_Type1') # IGBP classification band\n",
        "\n",
        "        # Get the image for the year (should be one)\n",
        "        land_cover_image = land_cover_collection.first()\n",
        "\n",
        "        # A robust check to see if the image is valid and has bands\n",
        "        # Attempt to get band names; if it fails or is empty, image is likely invalid/empty\n",
        "        valid_image = False\n",
        "        try:\n",
        "            if land_cover_image.bandNames().size().getInfo() > 0:\n",
        "                valid_image = True\n",
        "        except Exception: # Handles cases where land_cover_image might be null or not a proper image\n",
        "            valid_image = False\n",
        "\n",
        "        if not valid_image:\n",
        "             print(f\"No MODIS land cover data found or image is invalid for year {year_str}.\")\n",
        "             data_for_df = {'Year': year_str, 'Cobertura_Terrestre_Principal': np.nan, 'Cobertura_Terrestre_Porcentaje': np.nan}\n",
        "        else:\n",
        "            # Calculate frequency histogram of land cover types in the region\n",
        "            histogram = land_cover_image.reduceRegion(\n",
        "                reducer=ee.Reducer.frequencyHistogram(),\n",
        "                geometry=ee_region,\n",
        "                scale=scale,\n",
        "                maxPixels=1e10,\n",
        "                bestEffort=True,\n",
        "                tileScale=0.1\n",
        "            ).get('LC_Type1') # Get the histogram for the band\n",
        "\n",
        "            histogram_info = histogram.getInfo() # This can be slow for very large regions\n",
        "\n",
        "            if not histogram_info: # Check if histogram is empty\n",
        "                print(f\"Land cover histogram is empty for year {year_str} in the region.\")\n",
        "                data_for_df = {'Year': year_str, 'Cobertura_Terrestre_Principal': np.nan, 'Cobertura_Terrestre_Porcentaje': np.nan}\n",
        "            else:\n",
        "                # Convert keys (class IDs) to integers and find the principal class\n",
        "                class_counts = {int(k): v for k, v in histogram_info.items()}\n",
        "                total_pixels = sum(class_counts.values())\n",
        "\n",
        "                if total_pixels == 0:\n",
        "                    principal_class_id = np.nan\n",
        "                    percentage = np.nan\n",
        "                else:\n",
        "                    principal_class_id = max(class_counts, key=class_counts.get)\n",
        "                    percentage = (class_counts[principal_class_id] / total_pixels) * 100\n",
        "\n",
        "                data_for_df = {\n",
        "                    'Year': year_str,\n",
        "                    'Cobertura_Terrestre_Principal': principal_class_id,\n",
        "                    'Cobertura_Terrestre_Porcentaje': percentage\n",
        "                }\n",
        "\n",
        "        df = pd.DataFrame([data_for_df])\n",
        "        csv_filename = f\"{variable_name_prefix}{year_str}.csv\"\n",
        "        csv_path = os.path.join(output_dir, csv_filename)\n",
        "        df.to_csv(csv_path, index=False, na_rep=str(np.nan))\n",
        "\n",
        "        print(f\"Successfully saved land cover data for {year_str} to {csv_path}\")\n",
        "        return csv_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting land cover data for {year_str}: {e}\")\n",
        "        df = pd.DataFrame([{'Year': year_str, 'Cobertura_Terrestre_Principal': np.nan, 'Cobertura_Terrestre_Porcentaje': np.nan}])\n",
        "        csv_filename = f\"{variable_name_prefix}{year_str}_error.csv\"\n",
        "        csv_path = os.path.join(output_dir, csv_filename)\n",
        "        df.to_csv(csv_path, index=False, na_rep=str(np.nan))\n",
        "        return csv_path\n"
      ],
      "id": "kd8QV5hk3cAe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7KU5Tzw3cAe"
      },
      "source": [
        "---\n",
        "## Example Usage of Extraction Functions\n",
        "\n",
        "Below are examples of how to use the implemented functions.\n",
        "- You'll need to define your `region_of_interest` (as a GeoJSON-like Python dictionary).\n",
        "- Specify your desired `start_date`, `end_date`, `year_for_landcover`, and `output_directory`.\n",
        "- Uncomment the function calls you wish to run.\n",
        "- Ensure the `output_directory` exists or the functions will create it."
      ],
      "id": "A7KU5Tzw3cAe"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import re\n",
        "\n",
        "def dms_to_decimal(coord_str):\n",
        "    # First, try to parse as a decimal degree with optional direction\n",
        "    decimal_pattern = re.compile(\n",
        "        r'^\\s*([+-]?[\\d.]+)\\s*([NSEW]?)\\s*$',\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "    decimal_match = decimal_pattern.match(coord_str.strip())\n",
        "    if decimal_match:\n",
        "        number_str, direction = decimal_match.groups()\n",
        "        try:\n",
        "            number = float(number_str)\n",
        "        except ValueError:\n",
        "            pass  # Not a valid decimal, proceed to check DMS\n",
        "        else:\n",
        "            if direction:\n",
        "                direction = direction.upper()\n",
        "                decimal = abs(number)\n",
        "                if direction in ['S', 'W']:\n",
        "                    decimal *= -1\n",
        "                return decimal\n",
        "            else:\n",
        "                return number\n",
        "\n",
        "    # If not a decimal, attempt to parse as DMS\n",
        "    dms_pattern = re.compile(\n",
        "        r'''\\s*([+-]?\\d+)\\s*°\\s*([+-]?\\d+)\\s*'\\s*([+-]?\\d+\\.?\\d*)\\s*\"*\\s*([NSEW]?)\\s*''',\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "    dms_match = dms_pattern.match(coord_str.strip())\n",
        "    if not dms_match:\n",
        "        raise ValueError(f\"Invalid coordinate format: {coord_str}\")\n",
        "\n",
        "    degrees, minutes, seconds, direction = dms_match.groups()\n",
        "    degrees = float(degrees)\n",
        "    minutes = float(minutes)\n",
        "    seconds = float(seconds)\n",
        "    decimal = degrees + minutes / 60 + seconds / 3600\n",
        "\n",
        "    if direction.upper() in ['S', 'W']:\n",
        "        decimal *= -1\n",
        "\n",
        "    return decimal\n",
        "\n",
        "def get_point(center_lat, center_lon, distance_km, bearing_deg):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    delta = distance_km / R  # Angular distance in radians\n",
        "\n",
        "    lat1 = math.radians(center_lat)\n",
        "    lon1 = math.radians(center_lon)\n",
        "    theta = math.radians(bearing_deg)\n",
        "\n",
        "    lat2 = math.asin(\n",
        "        math.sin(lat1) * math.cos(delta) +\n",
        "        math.cos(lat1) * math.sin(delta) * math.cos(theta))\n",
        "    lon2 = lon1 + math.atan2(\n",
        "        math.sin(theta) * math.sin(delta) * math.cos(lat1),\n",
        "        math.cos(delta) - math.sin(lat1) * math.sin(lat2))\n",
        "\n",
        "    lon2 = (lon2 + 3 * math.pi) % (2 * math.pi) - math.pi\n",
        "\n",
        "    return (math.degrees(lat2), math.degrees(lon2))\n",
        "\n",
        "def generate_polygon_vertices(lat_input, lon_input, num_points, distance_km):\n",
        "    # Convert inputs to decimal degrees\n",
        "    try:\n",
        "        center_lat = dms_to_decimal(lat_input)\n",
        "        center_lon = dms_to_decimal(lon_input)\n",
        "    except ValueError as e:\n",
        "        raise ValueError(f\"Invalid coordinate format: {e}\")\n",
        "\n",
        "    vertices = []\n",
        "    for i in range(num_points):\n",
        "        bearing = (i * 360.0) / num_points\n",
        "        lat, lon = get_point(center_lat, center_lon, distance_km, bearing)\n",
        "        vertices.append((lat, lon))\n",
        "    return vertices"
      ],
      "metadata": {
        "id": "lA4EDcvK4RtT"
      },
      "id": "lA4EDcvK4RtT",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_polygon_vertices('16.697997', '42.794972', 4, 5)"
      ],
      "metadata": {
        "id": "_wI-tbtD5rN7",
        "outputId": "2785d085-56fe-4641-835e-171653f5727b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_wI-tbtD5rN7",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(16.742963080295937, 42.79497199999995),\n",
              " (16.697991706970537, 42.841917658644434),\n",
              " (16.65303091970406, 42.79497199999995),\n",
              " (16.697991706970537, 42.74802634135547)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_polygon_vertices(\"16° 41' 53\"\"\", \"42° 47' 42\"\"\", 4, 5)"
      ],
      "metadata": {
        "id": "N8uXimwU6gZS",
        "outputId": "5920337d-258c-4d2e-da2a-2f116324ec4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "N8uXimwU6gZS",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(16.74302163585149, 42.79499999999996),\n",
              " (16.698050262506435, 42.84194567303673),\n",
              " (16.653089475259613, 42.79499999999996),\n",
              " (16.698050262506435, 42.748054326963285)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_vertices_to_geojson(vertices):\n",
        "    \"\"\"\n",
        "    Convert polygon vertices to a GeoJSON-like Polygon dictionary.\n",
        "\n",
        "    Args:\n",
        "        vertices (list): List of (latitude, longitude) tuples\n",
        "\n",
        "    Returns:\n",
        "        dict: GeoJSON-like Polygon dictionary\n",
        "    \"\"\"\n",
        "    # Convert (latitude, longitude) to (longitude, latitude) for GeoJSON\n",
        "    coordinates = [[lon, lat] for lat, lon in vertices]\n",
        "\n",
        "    # Ensure the polygon is closed by appending the first coordinate if necessary\n",
        "    if coordinates:\n",
        "        if coordinates[0] != coordinates[-1]:\n",
        "            coordinates.append(coordinates[0])\n",
        "\n",
        "    return {\n",
        "        'type': 'Polygon',\n",
        "        'coordinates': [coordinates]\n",
        "    }"
      ],
      "metadata": {
        "id": "M99R4ClsB5O3"
      },
      "id": "M99R4ClsB5O3",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vertices = generate_polygon_vertices('16.697997', '42.794972', 4, 5)\n",
        "poligon_AOI = convert_vertices_to_geojson(vertices)\n",
        "poligon_AOI"
      ],
      "metadata": {
        "id": "yp4B5bUW7K2v",
        "outputId": "a206f62c-8ec3-469a-c9f6-1d8f4dd0de6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yp4B5bUW7K2v",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'Polygon',\n",
              " 'coordinates': [[[42.79497199999995, 16.742963080295937],\n",
              "   [42.841917658644434, 16.697991706970537],\n",
              "   [42.79497199999995, 16.65303091970406],\n",
              "   [42.74802634135547, 16.697991706970537],\n",
              "   [42.79497199999995, 16.742963080295937]]]}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "47a1iGmpF6bx"
      },
      "id": "47a1iGmpF6bx"
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "from folium import Figure\n",
        "from html2image import Html2Image\n",
        "\n",
        "def visualize_with_folium(vertices, center_lat=None, center_lon=None,\n",
        "                          zoom_start=12, save_path=\"map.html\"):\n",
        "    \"\"\"\n",
        "    Creates a Folium map with a polygon and saves it as an HTML file.\n",
        "\n",
        "    :param vertices: List of [lat, lon] pairs defining the polygon\n",
        "    :param center_lat: Center latitude (optional)\n",
        "    :param center_lon: Center longitude (optional)\n",
        "    :param zoom_start: Initial zoom level\n",
        "    :param save_path: Path to save the HTML file\n",
        "    :return: Folium Map object\n",
        "    \"\"\"\n",
        "    # Calculate center if not provided\n",
        "    if center_lat is None or center_lon is None:\n",
        "        center_lat = sum(v[0] for v in vertices) / len(vertices)\n",
        "        center_lon = sum(v[1] for v in vertices) / len(vertices)\n",
        "\n",
        "    # Create a Figure container\n",
        "    figure = Figure(width=800, height=600)\n",
        "\n",
        "    # Create base map inside the figure\n",
        "    map_obj = folium.Map(\n",
        "        location=[center_lat, center_lon],\n",
        "        zoom_start=zoom_start,\n",
        "        tiles='Esri.WorldImagery'\n",
        "    )\n",
        "    figure.add_child(map_obj)\n",
        "\n",
        "    # Add elements\n",
        "    folium.Marker(\n",
        "        [center_lat, center_lon],\n",
        "        popup=\"Center\",\n",
        "        icon=folium.Icon(color='red')\n",
        "    ).add_to(map_obj)\n",
        "\n",
        "    folium.Polygon(\n",
        "        vertices,\n",
        "        color='#ff0000',\n",
        "        fill=True,\n",
        "        fill_opacity=0.2\n",
        "    ).add_to(map_obj)\n",
        "\n",
        "    # Save HTML\n",
        "    map_obj.save(save_path)\n",
        "\n",
        "    return map_obj"
      ],
      "metadata": {
        "id": "BeSEYgLu_ULD"
      },
      "id": "BeSEYgLu_ULD",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AKjR-MQ8HoAm"
      },
      "id": "AKjR-MQ8HoAm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate vertices\n",
        "vertices = generate_polygon_vertices(\n",
        "    lat_input=\"24° 31' 33\\\" N\",\n",
        "    lon_input=\"40° 44' 48\\\" E\",\n",
        "    num_points=4,\n",
        "    distance_km=5\n",
        ")\n",
        "\n",
        "# Define paths\n",
        "png_output_dir = os.getcwd()\n",
        "png_file_path = os.path.join(png_output_dir, \"map.png\")\n",
        "html_map_path = os.path.join(png_output_dir,\"map.html\")\n",
        "\n",
        "\n",
        "# Create visualization and save HTML\n",
        "map_obj = visualize_with_folium(\n",
        "    vertices=vertices,\n",
        "    save_path=html_map_path,\n",
        "    zoom_start=12\n",
        ")\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(png_output_dir, exist_ok=True)\n",
        "\n",
        "# Initialize Html2Image\n",
        "# The previous initialization hti = Html2Image() is sufficient if wkhtmltoimage is in PATH.\n",
        "# The 'wkhtmltoimage' keyword argument is not accepted by the constructor.\n",
        "# If wkhtmltoimage is not found, consider adding it to your system's PATH or\n",
        "# setting the WKHTMLTOIMAGE_PATH environment variable.\n",
        "hti = Html2Image(output_path=png_output_dir)\n",
        "\n",
        "# Generate the PNG from the saved HTML using Html2Image\n",
        "hti.screenshot(\n",
        "    html_file=html_map_path,\n",
        "    save_as='map.png',\n",
        "    size=(800, 600)\n",
        ")\n",
        "\n",
        "print(f\"PNG file has been saved to: {png_file_path}\")"
      ],
      "metadata": {
        "id": "RFUzsxWT_XcL",
        "outputId": "2e8e2651-6d65-4474-cd4c-82adffeab24b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RFUzsxWT_XcL",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PNG file has been saved to: /content/map.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "import os\n",
        "import nest_asyncio # Import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to allow asyncio to run inside a loop (like Jupyter)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def take_screenshot(html_path, output_png_path, width=800, height=600):\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch() # Or firefox, webkit\n",
        "\n",
        "        # --- Add this line to create a BrowserContext ---\n",
        "        context = await browser.new_context(viewport={'width': width, 'height': height})\n",
        "        # --- End of added line ---\n",
        "\n",
        "        # --- Modify this line to create the page from the context ---\n",
        "        page = await context.new_page() # Create the page within the context\n",
        "        # --- End of modified line ---\n",
        "\n",
        "        # Construct file URL\n",
        "        file_url = f'file://{os.path.abspath(html_path)}'\n",
        "        print(f\"Attempting to load HTML file from: {file_url}\") # Debugging print\n",
        "\n",
        "        try:\n",
        "            await page.goto(file_url)\n",
        "            print(f\"Successfully loaded {html_path}\")\n",
        "            await page.screenshot(path=output_png_path)\n",
        "            print(f\"Screenshot saved to: {output_png_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during screenshot: {e}\")\n",
        "        finally:\n",
        "            # --- Close the context and then the browser ---\n",
        "            await context.close() # Close the context\n",
        "            await browser.close() # Close the browser\n",
        "            # --- End of closing lines ---\n",
        "            print(\"Browser and context closed.\")\n"
      ],
      "metadata": {
        "id": "zHrVYkkHQblv"
      },
      "id": "zHrVYkkHQblv",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now run the async function\n",
        "# You can use asyncio.run() after applying nest_asyncio, or the %autoawait magic\n",
        "\n",
        "# Using asyncio.run()\n",
        "try:\n",
        "    asyncio.run(take_screenshot(html_map_path, png_file_path, width=800, height=600))\n",
        "except Exception as e:\n",
        "     print(f\"Error running asyncio task: {e}\")\n"
      ],
      "metadata": {
        "id": "m5Ba4WWxR15n",
        "outputId": "25f0546f-2b49-49f2-83ae-1ab22171492b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "m5Ba4WWxR15n",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load HTML file from: file:///content/map.html\n",
            "Successfully loaded /content/map.html\n",
            "Screenshot saved to: /content/map.png\n",
            "Browser and context closed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "PKI8nHbD3cAe",
        "outputId": "de9aa728-5a55-46a3-ca08-47b145886057",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No images found for Albedo_BSA in period 2000-01-01 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-02 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-03 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-04 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-05 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-06 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-07 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-08 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-09 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-10 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-11 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-12 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-13 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-14 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-15 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-16 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-17 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-18 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-19 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-20 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-21 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-22 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-23 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-24 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-25 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-26 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-27 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-28 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-29 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-30 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-01-31 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-01 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-02 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-03 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-04 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-05 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-06 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-07 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-08 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-09 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-10 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-11 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-12 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-13 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-14 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-15 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-16 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-17 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-18 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-19 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-20 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-21 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-22 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2000-02-23 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2001-06-24 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_BSA in period 2001-06-25 for dataset MODIS/061/MCD43A3\n",
            "Successfully saved data for Albedo_BSA to gee_output_data/Albedo_BSA_daily_20000101_20231130.csv\n",
            "--- Extracting Albedo WSA (daily) ---\n",
            "No images found for Albedo_WSA in period 2000-01-01 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-02 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-03 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-04 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-05 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-06 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-07 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-08 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-09 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-10 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-11 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-12 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-13 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-14 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-15 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-16 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-17 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-18 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-19 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-20 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-21 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-22 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-23 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-24 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-25 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-26 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-27 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-28 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-29 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-30 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-01-31 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-01 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-02 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-03 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-04 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-05 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-06 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-07 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-08 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-09 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-10 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-11 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-12 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-13 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-14 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-15 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-16 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-17 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-18 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-19 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-20 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-21 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-22 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2000-02-23 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2001-06-24 for dataset MODIS/061/MCD43A3\n",
            "No images found for Albedo_WSA in period 2001-06-25 for dataset MODIS/061/MCD43A3\n",
            "Successfully saved data for Albedo_WSA to gee_output_data/Albedo_WSA_daily_20000101_20231130.csv\n"
          ]
        }
      ],
      "source": [
        "# --- Example Parameters (USER: PLEASE DEFINE THESE) ---\n",
        "\n",
        "# Define your Region of Interest (AOI) as a GeoJSON-like dictionary.\n",
        "# Example: A small rectangle in an arbitrary location.\n",
        "# REPLACE with your actual coordinates.\n",
        "region_of_interest = poligon_AOI\n",
        "\n",
        "# Define your date range for time-series data\n",
        "example_start_date = '2000-01-01'\n",
        "example_end_date = '2023-11-30' # For a shorter example run; extend as needed\n",
        "\n",
        "# Define the year for land cover analysis\n",
        "example_year_lc = '2020'\n",
        "\n",
        "# Define the output directory for CSV files\n",
        "output_csv_directory = 'gee_output_data'\n",
        "# The functions will attempt to create this directory if it doesn't exist.\n",
        "\n",
        "# Define the desired frequency for time-series aggregation\n",
        "# Options: 'hourly', 'daily', 'monthly', 'yearly'\n",
        "example_frequency = 'daily'\n",
        "\n",
        "# --- Ensure output directory exists ---\n",
        "os.makedirs(output_csv_directory, exist_ok=True) # Generic function handles this\n",
        "\n",
        "# Extracting Albedo BSA\n",
        "# print(f\"--- Extracting Albedo BSA ({example_frequency}) ---\")\n",
        "bsa_path = extract_albedo_bsa(\n",
        "    region_geojson=region_of_interest,\n",
        "    start_date_str=example_start_date,\n",
        "    end_date_str=example_end_date,\n",
        "    frequency=example_frequency,\n",
        "    output_dir=output_csv_directory\n",
        ")\n",
        "\n",
        "# Extracting Albedo WSA\n",
        "print(f\"--- Extracting Albedo WSA ({example_frequency}) ---\")\n",
        "wsa_path = extract_albedo_wsa(\n",
        "    region_geojson=region_of_interest,\n",
        "    start_date_str=example_start_date,\n",
        "    end_date_str=example_end_date,\n",
        "    frequency=example_frequency,\n",
        "    output_dir=output_csv_directory\n",
        ")\n",
        "\n",
        "# print(f\"--- Extracting Approximated Solar Radiation ({example_frequency}) ---\")\n",
        "# extract_radiacion_solar(\n",
        "#     region_geojson=region_of_interest,\n",
        "#     start_date_str=example_start_date,\n",
        "#     end_date_str=example_end_date,\n",
        "#     frequency=example_frequency,\n",
        "#     output_dir=output_csv_directory\n",
        "# )\n",
        "\n",
        "# print(f\"--- Extracting Day Temperature ({example_frequency}) ---\")\n",
        "# extract_temperatura_dia(\n",
        "#     region_geojson=region_of_interest,\n",
        "#     start_date_str=example_start_date,\n",
        "#     end_date_str=example_end_date,\n",
        "#     frequency=example_frequency,\n",
        "#     output_dir=output_csv_directory\n",
        "# )\n",
        "\n",
        "# print(f\"--- Extracting Night Temperature ({example_frequency}) ---\")\n",
        "# extract_temperatura_noche(\n",
        "#     region_geojson=region_of_interest,\n",
        "#     start_date_str=example_start_date,\n",
        "#     end_date_str=example_end_date,\n",
        "#     frequency=example_frequency,\n",
        "#     output_dir=output_csv_directory\n",
        "# )\n",
        "\n",
        "# print(f\"--- Extracting Wind Data ({example_frequency}) ---\")\n",
        "# # Note: Wind data from ERA5 is hourly. Generic function will average u/v components\n",
        "# # to the target frequency before calculating speed/direction.\n",
        "# extract_viento(\n",
        "#     region_geojson=region_of_interest,\n",
        "#     start_date_str=example_start_date,\n",
        "#     end_date_str=example_end_date,\n",
        "#     frequency=example_frequency, # e.g., 'daily' will average hourly components to daily means first\n",
        "#     output_dir=output_csv_directory\n",
        "# )\n",
        "\n",
        "# print(f\"--- Extracting Topography Data (Static) ---\")\n",
        "# extract_topography(\n",
        "#     region_geojson=region_of_interest,\n",
        "#     output_dir=output_csv_directory\n",
        "# )\n",
        "\n",
        "# print(f\"--- Extracting Land Cover Data for {example_year_lc} ---\")\n",
        "# extract_land_cover(\n",
        "#     region_geojson=region_of_interest,\n",
        "#     year_str=example_year_lc,\n",
        "#     output_dir=output_csv_directory\n",
        "# )\n",
        "\n"
      ],
      "id": "PKI8nHbD3cAe"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reportlab\n",
        "!pip install html2image\n"
      ],
      "metadata": {
        "id": "sbrPxnIVXDYa",
        "outputId": "19677516-cd7a-4982-8b04-c03342acc907",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sbrPxnIVXDYa",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting html2image\n",
            "  Downloading html2image-2.0.7-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from html2image) (2.32.3)\n",
            "Requirement already satisfied: websocket-client~=1.0 in /usr/local/lib/python3.11/dist-packages (from html2image) (1.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->html2image) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->html2image) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->html2image) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->html2image) (2025.4.26)\n",
            "Downloading html2image-2.0.7-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: html2image\n",
            "Successfully installed html2image-2.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.units import inch\n",
        "from datetime import datetime\n",
        "\n",
        "# Convert HTML map to PNG and add to report\n",
        "from html2image import Html2Image\n",
        "\n",
        "# 1. Load and preprocess data\n",
        "def load_data(bsa_path, wsa_path):\n",
        "    bsa = pd.read_csv(bsa_path, parse_dates=['timestamp'])\n",
        "    wsa = pd.read_csv(wsa_path, parse_dates=['timestamp'])\n",
        "    df = pd.merge(bsa, wsa, on='timestamp', suffixes=('_BSA', '_WSA'))\n",
        "    df['month'] = df['timestamp'].dt.month\n",
        "    df['year'] = df['timestamp'].dt.year\n",
        "    return df\n",
        "\n",
        "# 2. Generate statistics tables\n",
        "def calculate_statistics(df):\n",
        "    monthly_stats = df.groupby('month')['Albedo_BSA'].agg([\n",
        "        'mean', 'min', 'max',\n",
        "        lambda x: x.quantile(0.01),  # p1\n",
        "        lambda x: x.quantile(0.05),  # p5\n",
        "        lambda x: x.quantile(0.10),  # p10\n",
        "        lambda x: x.quantile(0.25),  # p25\n",
        "        lambda x: x.quantile(0.50),  # median\n",
        "        lambda x: x.quantile(0.75),  # p75\n",
        "        lambda x: x.quantile(0.90),  # p90\n",
        "        lambda x: x.quantile(0.95),  # p95\n",
        "        lambda x: x.quantile(0.99),  # p99\n",
        "    ])\n",
        "    monthly_stats.columns = ['average', 'min', 'max', 'p1', 'p5', 'p10', 'p25',\n",
        "                            'p50', 'p75', 'p90', 'p95', 'p99']\n",
        "    return monthly_stats\n",
        "\n",
        "# 3. Create visualizations\n",
        "def create_figures(df, monthly_stats):\n",
        "    # Figure 1: Monthly statistics\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    monthly_stats[['average', 'min', 'max']].plot(kind='line', marker='o')\n",
        "    plt.title('Monthly Albedo Statistics')\n",
        "    plt.ylabel('Albedo')\n",
        "    plt.xlabel('Month')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('monthly_stats.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 2: Daily values timeline\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df['timestamp'], df['Albedo_BSA'], label='Daily Albedo')\n",
        "    plt.title('Daily Albedo Values')\n",
        "    plt.ylabel('Albedo')\n",
        "    plt.xlabel('Date')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('daily_values.png')\n",
        "    plt.close()\n",
        "\n",
        "# 4. Generate PDF report\n",
        "def create_pdf_report(monthly_stats, annual_avg):\n",
        "    doc = SimpleDocTemplate(\"Albedo_Report.pdf\", pagesize=letter)\n",
        "    styles = getSampleStyleSheet()\n",
        "    elements = []\n",
        "\n",
        "    # Title\n",
        "    elements.append(Paragraph(\"Albedo Analysis Report\", styles['Title']))\n",
        "    elements.append(Spacer(1, 0.25*inch))\n",
        "\n",
        "    # Add map image\n",
        "    coordinates_text = str(poligon_AOI['coordinates'])\n",
        "    elements.append(Paragraph(\"Study Area Location\", styles['Heading2']))\n",
        "    elements.append(Paragraph(coordinates_text, styles['BodyText']))\n",
        "    elements.append(Image('map.png', width=6*inch, height=4*inch))\n",
        "    elements.append(Spacer(1, 0.25*inch))\n",
        "\n",
        "    # Methodology\n",
        "    methodology = \"\"\"\n",
        "    <para align=justify>\n",
        "    The final albedo was calculated using MODIS MCD43A3 data (Collection 6.1) including both\n",
        "    black-sky albedo (BSA) and white-sky albedo (WSA) values. The analysis combines daily\n",
        "    measurements from {start_year} to {end_year}, aggregated to monthly and annual statistics.\n",
        "    The dataset shows albedo values ranging between {min_val:.2f} and {max_val:.2f} with an\n",
        "    annual average of {avg_val:.2f}.\n",
        "    </para>\n",
        "    \"\"\".format(\n",
        "        start_year=df['timestamp'].min().year,\n",
        "        end_year=df['timestamp'].max().year,\n",
        "        min_val=df['Albedo_BSA'].min(),\n",
        "        max_val=df['Albedo_BSA'].max(),\n",
        "        avg_val=annual_avg\n",
        "    )\n",
        "    elements.append(Paragraph(methodology, styles['BodyText']))\n",
        "    elements.append(Spacer(1, 0.25*inch))\n",
        "\n",
        "    # Monthly Statistics Table\n",
        "    table_data = [['Month', 'Avg', 'Min', 'Max', 'P10', 'P50', 'P90']]\n",
        "    for month, data in monthly_stats.iterrows():\n",
        "        table_data.append([\n",
        "            str(month),\n",
        "            f\"{data['average']:.2f}\",\n",
        "            f\"{data['min']:.2f}\",\n",
        "            f\"{data['max']:.2f}\",\n",
        "            f\"{data['p10']:.2f}\",\n",
        "            f\"{data['p50']:.2f}\",\n",
        "            f\"{data['p90']:.2f}\"\n",
        "        ])\n",
        "\n",
        "    t = Table(table_data)\n",
        "    t.setStyle(TableStyle([\n",
        "        ('BACKGROUND', (0,0), (-1,0), colors.grey),\n",
        "        ('TEXTCOLOR', (0,0), (-1,0), colors.whitesmoke),\n",
        "        ('ALIGN', (0,0), (-1,-1), 'CENTER'),\n",
        "        ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n",
        "        ('FONTSIZE', (0,0), (-1,0), 12),\n",
        "        ('BOTTOMPADDING', (0,0), (-1,0), 12),\n",
        "        ('BACKGROUND', (0,1), (-1,-1), colors.beige),\n",
        "        ('GRID', (0,0), (-1,-1), 1, colors.black)\n",
        "    ]))\n",
        "    elements.append(t)\n",
        "    elements.append(Spacer(1, 0.5*inch))\n",
        "\n",
        "    # Add figures\n",
        "    elements.append(Paragraph(\"Monthly Albedo Statistics\", styles['Heading2']))\n",
        "    elements.append(Image('monthly_stats.png', width=6*inch, height=4*inch))\n",
        "    elements.append(Spacer(1, 0.25*inch))\n",
        "\n",
        "    elements.append(Paragraph(\"Daily Albedo Values Timeline\", styles['Heading2']))\n",
        "    elements.append(Image('daily_values.png', width=6*inch, height=4*inch))\n",
        "\n",
        "    doc.build(elements)\n",
        "\n",
        "# Main workflow\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your CSV files\n",
        "    try:\n",
        "      df = load_data(bsa_path, wsa_path)\n",
        "    except:\n",
        "      bsa_path = '/content/gee_output_data/Albedo_BSA_daily_20000101_20231130.csv'\n",
        "      wsa_path = '/content/gee_output_data/Albedo_WSA_daily_20000101_20231130.csv'\n",
        "      df = load_data(bsa_path, wsa_path)\n",
        "\n",
        "    # Calculate statistics\n",
        "    monthly_stats = calculate_statistics(df)\n",
        "    annual_avg = df['Albedo_BSA'].mean()\n",
        "\n",
        "    # Generate visualizations\n",
        "    create_figures(df, monthly_stats)\n",
        "\n",
        "    # Create PDF report\n",
        "    create_pdf_report(monthly_stats, annual_avg)"
      ],
      "metadata": {
        "id": "kHA3qVUSV_92",
        "outputId": "8c1fdd7d-027d-413f-a299-5e73807f90b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "id": "kHA3qVUSV_92",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}