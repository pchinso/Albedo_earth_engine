{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbcwRI4e3cAc"
      },
      "source": [
        "# Google Earth Engine Variable Extraction Notebook\n",
        "\n",
        "This notebook provides functions to extract various environmental variables from Google Earth Engine (GEE) datasets.\n",
        "The primary goals are:\n",
        "- Define functions to retrieve specific variables based on user-defined parameters (region, time period, frequency).\n",
        "- For time-series data, aggregate it to hourly, daily, monthly, or yearly means.\n",
        "- Save the extracted data for each variable into a separate CSV file.\n",
        "\n",
        "**Instructions:**\n",
        "1. Run the GEE Authentication and Initialization cell first. You will need to authenticate with a Google account that has GEE access.\n",
        "2. Define your region of interest (AOI) as a GeoJSON-like dictionary.\n",
        "3. Call the specific extraction functions for the variables you need, providing the AOI, date range, frequency, and output directory."
      ],
      "id": "JbcwRI4e3cAc"
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIG VALUES\n",
        "# BELA BELA -24.868138, 28.374596\n",
        "# Zona sin vegetacion '''24°52'48\"S 28°24'43\"E'''  '''24°52'37\"S 28°24'07\"E'''\n",
        "# Zona con vegetacion '''24°52'12\"S 28°22'26\"E'''\n",
        "\n",
        "# AlHenakiyah (SaudiArabia)\n",
        "# 24.52594, 40.74656\n",
        "\n",
        "# Location\n",
        "LOCATION_NAME = 'BELA BELA'\n",
        "LATITUDE = '''24°52'48\"S'''\n",
        "LONGITUDE =  '''28°24'07\"E'''\n",
        "DESCRIPCION = 'Zona sin vegetacion'\n",
        "\n",
        "# Area of interest\n",
        "POLIGON_VERTICES = 6\n",
        "RADIO = 0.5 # Kilometers\n",
        "\n",
        "# PERIOD\n",
        "INITIAL_DATE = '2023-01-01'\n",
        "FINAL_DATE = '2023-11-30'\n",
        "# Define the desired frequency for time-series aggregation in csv file\n",
        "# Options: 'hourly', 'daily', 'monthly', 'yearly'\n",
        "FRECUENCY = 'daily'"
      ],
      "metadata": {
        "id": "rIgAF9AEAScM"
      },
      "id": "rIgAF9AEAScM",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this once per session\n",
        "# Update folium to latest version\n",
        "\n",
        "!sudo apt-get install wkhtmltopdf\n",
        "!pip install folium html2image\n",
        "!pip install --upgrade folium\n",
        "!pip install playwright\n",
        "!playwright install\n",
        "!pip install reportlab\n",
        "!pip install html2image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vez1d0dvFZDD",
        "outputId": "52f6545a-35dc-482a-9c32-ff524eaa03ce"
      },
      "id": "vez1d0dvFZDD",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "wkhtmltopdf is already the newest version (0.12.6-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
            "Requirement already satisfied: html2image in /usr/local/lib/python3.11/dist-packages (2.0.7)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from folium) (0.8.1)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from folium) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from folium) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from folium) (2.32.3)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.11/dist-packages (from folium) (2025.4.0)\n",
            "Requirement already satisfied: websocket-client~=1.0 in /usr/local/lib/python3.11/dist-packages (from html2image) (1.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.9->folium) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (2025.4.26)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from folium) (0.8.1)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from folium) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from folium) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from folium) (2.32.3)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.11/dist-packages (from folium) (2025.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.9->folium) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->folium) (2025.4.26)\n",
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.52.0)\n",
            "Requirement already satisfied: pyee<14,>=13 in /usr/local/lib/python3.11/dist-packages (from playwright) (13.0.0)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<14,>=13->playwright) (4.13.2)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:269:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:927:14)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:1047:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:1036:7)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:160:7)\n",
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.11/dist-packages (4.4.1)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from reportlab) (11.2.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from reportlab) (5.2.0)\n",
            "Requirement already satisfied: html2image in /usr/local/lib/python3.11/dist-packages (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from html2image) (2.32.3)\n",
            "Requirement already satisfied: websocket-client~=1.0 in /usr/local/lib/python3.11/dist-packages (from html2image) (1.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->html2image) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->html2image) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->html2image) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->html2image) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "SHiLFHIt3cAd"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import os\n",
        "import csv\n",
        "import math # Ensure math is imported for potential calculations like wind\n",
        "import calendar"
      ],
      "id": "SHiLFHIt3cAd"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_VI4WPE3cAd",
        "outputId": "95b0003a-64e6-4af0-da7c-089642a9d423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GEE Initialized successfully.\n",
            "GEE is initialized and accessible.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# Trigger the authentication flow.\n",
        "# This will print a URL, open it, authorize, and copy the code back into the input box.\n",
        "try:\n",
        "    ee.Authenticate()\n",
        "except Exception as e:\n",
        "    print(f\"Authentication failed or already authenticated: {e}\")\n",
        "    # For automated environments or if auth was done in a previous session,\n",
        "    # this might raise an error if not needed, so we can often proceed.\n",
        "\n",
        "# Initialize the library.\n",
        "# Replace 'YOUR_GEE_PROJECT' with your actual GEE project ID if you have one,\n",
        "# otherwise, GEE often can use a default cloud project associated with your account.\n",
        "try:\n",
        "    ee.Initialize(project='gen-lang-client-0253961861')\n",
        "    print(\"GEE Initialized successfully.\")\n",
        "except Exception as e:\n",
        "    try:\n",
        "        # Fallback if project-specific initialization fails\n",
        "        ee.Initialize()\n",
        "        print(\"GEE Initialized successfully (default project).\")\n",
        "    except Exception as e_init:\n",
        "        print(f\"GEE Initialization failed: {e_init}\")\n",
        "        print(\"Please ensure you have authenticated and have a GEE-enabled project.\")\n",
        "\n",
        "# Define a helper to check GEE initialization status\n",
        "def check_gee_initialized():\n",
        "    try:\n",
        "        ee.ImageCollection('MODIS/061/MCD43A3').limit(1).size().getInfo()\n",
        "        print(\"GEE is initialized and accessible.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"GEE not properly initialized or accessible: {e}\")\n",
        "        return False\n",
        "\n",
        "check_gee_initialized()"
      ],
      "id": "0_VI4WPE3cAd"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "XHlRJn9Y3cAd"
      },
      "outputs": [],
      "source": [
        "# Generic Time Series Extraction Function\n",
        "def extract_gee_time_series(\n",
        "    variable_name: str,\n",
        "    region_geojson: dict, # GeoJSON dictionary for the region\n",
        "    start_date_str: str,\n",
        "    end_date_str: str,\n",
        "    frequency: str,  # 'hourly', 'daily', 'monthly', 'yearly'\n",
        "    gee_dataset_id: str,\n",
        "    gee_band_name: str, # Can be a list for multi-band calculations (e.g. wind)\n",
        "    scale: int,\n",
        "    output_dir: str,\n",
        "    gee_project: str = None, # Optional: GEE project for initialization if needed\n",
        "    reducer: ee.Reducer = ee.Reducer.mean(),\n",
        "    data_scaling_factor: float = None,\n",
        "    data_offset_factor: float = None,\n",
        "    post_process_function: callable = None, # Function to apply to the reduced value or dictionary of values\n",
        "    nan_value = None # Value to use if GEE returns None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Extracts time-series data from Google Earth Engine for a specified variable and frequency,\n",
        "    saves it to a CSV file, and returns the path to the CSV.\n",
        "\n",
        "    Parameters:\n",
        "    - variable_name: Name of the variable (used for CSV filename).\n",
        "    - region_geojson: GeoJSON dictionary defining the region of interest.\n",
        "    - start_date_str: Start date in 'YYYY-MM-DD' format.\n",
        "    - end_date_str: End date in 'YYYY-MM-DD' format.\n",
        "    - frequency: Aggregation frequency ('hourly', 'daily', 'monthly', 'yearly').\n",
        "    - gee_dataset_id: Earth Engine ImageCollection ID.\n",
        "    - gee_band_name: Name of the band(s) to extract. If a list, post_process_function must handle it.\n",
        "    - scale: Spatial resolution in meters for reduction.\n",
        "    - output_dir: Directory to save the output CSV file.\n",
        "    - gee_project: Optional GEE project ID for ee.Initialize().\n",
        "    - reducer: Earth Engine reducer to apply (default: ee.Reducer.mean()).\n",
        "    - data_scaling_factor: Optional factor to multiply the band data by.\n",
        "    - data_offset_factor: Optional offset to add to the band data.\n",
        "    - post_process_function: Optional function to apply to the raw reduced value(s).\n",
        "                             It should accept a dictionary of band values if multiple bands are processed,\n",
        "                             or a single value if one band is processed. It should return a dictionary\n",
        "                             of processed values or a single processed value.\n",
        "    - nan_value: Value to fill in if GEE returns no data for a period.\n",
        "\n",
        "    Returns:\n",
        "    - Path to the generated CSV file.\n",
        "    \"\"\"\n",
        "    if gee_project:\n",
        "        try:\n",
        "            ee.Initialize(project=gee_project)\n",
        "        except Exception:\n",
        "            ee.Initialize() # Fallback\n",
        "\n",
        "    ee_region = ee.Geometry(region_geojson)\n",
        "\n",
        "    start_date = datetime.datetime.strptime(start_date_str, '%Y-%m-%d')\n",
        "    end_date = datetime.datetime.strptime(end_date_str, '%Y-%m-%d')\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    date_periods = []\n",
        "    current_date = start_date\n",
        "\n",
        "    if frequency == 'hourly':\n",
        "        delta = relativedelta(hours=1)\n",
        "        date_format_label = '%Y-%m-%d %H:%M:%S'\n",
        "        while current_date <= end_date:\n",
        "            period_end_date = current_date + delta - relativedelta(seconds=1) # End of the hour\n",
        "            date_periods.append({\n",
        "                \"start\": current_date.strftime('%Y-%m-%dT%H:%M:%S'),\n",
        "                \"end\": period_end_date.strftime('%Y-%m-%dT%H:%M:%S'),\n",
        "                \"label\": current_date.strftime(date_format_label)\n",
        "            })\n",
        "            current_date += delta\n",
        "    elif frequency == 'daily':\n",
        "        delta = relativedelta(days=1)\n",
        "        date_format_label = '%Y-%m-%d'\n",
        "        while current_date <= end_date:\n",
        "            date_periods.append({\n",
        "                \"start\": current_date.strftime('%Y-%m-%d'),\n",
        "                \"end\": (current_date + delta - relativedelta(days=1)).strftime('%Y-%m-%d'), # inclusive end\n",
        "                \"label\": current_date.strftime(date_format_label)\n",
        "            })\n",
        "            current_date += delta\n",
        "    elif frequency == 'monthly':\n",
        "        delta = relativedelta(months=1)\n",
        "        date_format_label = '%Y-%m'\n",
        "        while current_date <= end_date:\n",
        "            month_start = current_date.replace(day=1)\n",
        "            month_end = month_start + delta - relativedelta(days=1)\n",
        "            date_periods.append({\n",
        "                \"start\": month_start.strftime('%Y-%m-%d'),\n",
        "                \"end\": month_end.strftime('%Y-%m-%d'),\n",
        "                \"label\": month_start.strftime(date_format_label)\n",
        "            })\n",
        "            current_date += delta\n",
        "    elif frequency == 'yearly':\n",
        "        delta = relativedelta(years=1)\n",
        "        date_format_label = '%Y'\n",
        "        while current_date <= end_date:\n",
        "            year_start = current_date.replace(month=1, day=1)\n",
        "            year_end = year_start + delta - relativedelta(days=1)\n",
        "            date_periods.append({\n",
        "                \"start\": year_start.strftime('%Y-%m-%d'),\n",
        "                \"end\": year_end.strftime('%Y-%m-%d'),\n",
        "                \"label\": year_start.strftime(date_format_label)\n",
        "            })\n",
        "            current_date += delta\n",
        "    else:\n",
        "        raise ValueError(\"Invalid frequency. Choose from 'hourly', 'daily', 'monthly', 'yearly'.\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for period in date_periods:\n",
        "        print(f\"Processing period {period['label']}\")\n",
        "        try:\n",
        "            collection = ee.ImageCollection(gee_dataset_id) \\\n",
        "                           .filterDate(ee.Date(period[\"start\"]), ee.Date(period[\"end\"]).advance(1, 'day')) # GEE end date is exclusive\n",
        "\n",
        "            if isinstance(gee_band_name, list): # For multi-band variables like wind\n",
        "                selected_bands_collection = collection.select(gee_band_name)\n",
        "            else: # Single band\n",
        "                selected_bands_collection = collection.select([gee_band_name])\n",
        "\n",
        "            # Check if collection is empty for the period\n",
        "            if selected_bands_collection.size().getInfo() == 0:\n",
        "                print(f\"No images found for {variable_name} in period {period['label']} for dataset {gee_dataset_id}\")\n",
        "                reduced_value = nan_value\n",
        "                if isinstance(gee_band_name, list) and nan_value is not None:\n",
        "                     # If multiple bands expected, fill with nan_value for each\n",
        "                    reduced_value = {band: nan_value for band in gee_band_name}\n",
        "\n",
        "                if post_process_function and reduced_value is not None : # nan_value can be processed if needed\n",
        "                     processed_value = post_process_function(reduced_value)\n",
        "                else:\n",
        "                     processed_value = reduced_value\n",
        "\n",
        "                # Ensure processed_value is a dictionary for DataFrame creation\n",
        "                if not isinstance(processed_value, dict) and isinstance(gee_band_name, list):\n",
        "                    # if single value came from post_process for multiple bands, try to map it\n",
        "                    # this might need adjustment based on post_process_function's behavior\n",
        "                    processed_value = {f\"{variable_name}_{b}\" if len(gee_band_name) > 1 else variable_name : processed_value for b in gee_band_name}\n",
        "                elif not isinstance(processed_value, dict):\n",
        "                    processed_value = {variable_name: processed_value}\n",
        "\n",
        "            else:\n",
        "                image_for_period = selected_bands_collection.mean() # Temporal aggregation\n",
        "\n",
        "                if data_scaling_factor is not None:\n",
        "                    image_for_period = image_for_period.multiply(data_scaling_factor)\n",
        "                if data_offset_factor is not None:\n",
        "                    image_for_period = image_for_period.add(data_offset_factor)\n",
        "\n",
        "                # Perform reduction\n",
        "                reduction = image_for_period.reduceRegion(\n",
        "                    reducer=reducer,\n",
        "                    geometry=ee_region,\n",
        "                    scale=scale,\n",
        "                    maxPixels=1e10,\n",
        "                    bestEffort=True, # Added bestEffort\n",
        "                    tileScale=0.1 # Added tileScale\n",
        "                )\n",
        "\n",
        "                raw_reduced_value = {}\n",
        "                if isinstance(gee_band_name, list):\n",
        "                    for band in gee_band_name:\n",
        "                        raw_reduced_value[band] = reduction.get(band).getInfo()\n",
        "                else:\n",
        "                    raw_reduced_value = reduction.get(gee_band_name).getInfo()\n",
        "\n",
        "                if post_process_function:\n",
        "                    processed_value = post_process_function(raw_reduced_value)\n",
        "                else:\n",
        "                    processed_value = raw_reduced_value\n",
        "\n",
        "            # Structure for DataFrame\n",
        "            current_row = {'timestamp': period['label']}\n",
        "            if isinstance(processed_value, dict):\n",
        "                current_row.update(processed_value)\n",
        "            else: # Single value result\n",
        "                current_row[variable_name] = processed_value\n",
        "            results.append(current_row)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing period {period['label']} for {variable_name}: {e}\")\n",
        "            # Add a row with nan_value or error indication\n",
        "            error_entry = {'timestamp': period['label']}\n",
        "            if isinstance(gee_band_name, list):\n",
        "                for band in gee_band_name:\n",
        "                    error_entry[f\"{variable_name}_{band}\"] = nan_value  # Or some error string\n",
        "            else:\n",
        "                error_entry[variable_name] = nan_value # Or some error string\n",
        "            results.append(error_entry)\n",
        "            continue # Continue to next period\n",
        "\n",
        "    if not results:\n",
        "        print(f\"No data extracted for {variable_name} in the given period.\")\n",
        "        return None\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    # Ensure timestamp is the first column\n",
        "    cols = ['timestamp'] + [col for col in df.columns if col != 'timestamp']\n",
        "    df = df[cols]\n",
        "\n",
        "    # Sanitize filename\n",
        "    safe_start_date = start_date_str.replace('-', '')\n",
        "    safe_end_date = end_date_str.replace('-', '')\n",
        "    csv_filename = f\"{variable_name}_{frequency}_{safe_start_date}_{safe_end_date}.csv\"\n",
        "    csv_path = os.path.join(output_dir, csv_filename)\n",
        "    df.to_csv(csv_path, index=False, na_rep=str(nan_value) if nan_value is not None else 'NaN') # Use nan_value for na_rep\n",
        "\n",
        "    print(f\"Successfully saved data for {variable_name} to {csv_path}\")\n",
        "    return csv_path\n",
        "\n",
        "# --- Helper function for MODIS LST to Celsius ---\n",
        "def convert_modis_lst_to_celsius(lst_value):\n",
        "    \"\"\"Converts MODIS LST (scaled Kelvin) to Celsius.\"\"\"\n",
        "    if lst_value is None:\n",
        "        return None\n",
        "    return (lst_value * 0.02) - 273.15\n",
        "\n",
        "# --- Specific Variable Extraction Functions ---\n",
        "\n",
        "# Albedo\n",
        "def extract_albedo_bsa(region_geojson, start_date_str, end_date_str, frequency, output_dir, scale=500):\n",
        "    return extract_gee_time_series(\n",
        "        variable_name=\"Albedo_BSA\",\n",
        "        region_geojson=region_geojson,\n",
        "        start_date_str=start_date_str,\n",
        "        end_date_str=end_date_str,\n",
        "        frequency=frequency,\n",
        "        gee_dataset_id='MODIS/061/MCD43A3', # As per reference notebook\n",
        "        gee_band_name='Albedo_BSA_shortwave', # As per reference notebook\n",
        "        scale=scale,\n",
        "        output_dir=output_dir,\n",
        "        data_scaling_factor=0.001, # MODIS albedo scaling\n",
        "        nan_value=np.nan # Use numpy's NaN for missing values\n",
        "    )\n",
        "\n",
        "def extract_albedo_wsa(region_geojson, start_date_str, end_date_str, frequency, output_dir, scale=500):\n",
        "    return extract_gee_time_series(\n",
        "        variable_name=\"Albedo_WSA\",\n",
        "        region_geojson=region_geojson,\n",
        "        start_date_str=start_date_str,\n",
        "        end_date_str=end_date_str,\n",
        "        frequency=frequency,\n",
        "        gee_dataset_id='MODIS/061/MCD43A3', # As per reference notebook\n",
        "        gee_band_name='Albedo_WSA_shortwave', # As per reference notebook\n",
        "        scale=scale,\n",
        "        output_dir=output_dir,\n",
        "        data_scaling_factor=0.001, # MODIS albedo scaling\n",
        "        nan_value=np.nan\n",
        "    )\n",
        "\n",
        "# Solar Radiation (using LST Day as proxy, similar to reference notebook)\n",
        "# Note: This is an approximation. A direct solar radiation dataset would be better if available and suitable.\n",
        "def post_process_solar_radiation(lst_day_value):\n",
        "    \"\"\"Approximates solar radiation from MODIS LST Day value.\"\"\"\n",
        "    if lst_day_value is None:\n",
        "        return None\n",
        "    # If input is a dict (from multi-band processing in generic func),\n",
        "    # extract the value. This handles cases where post_process_function\n",
        "    # might receive a dict even for single-band selection if the generic\n",
        "    # function's internal logic changes or if it's called directly with a dict.\n",
        "    if isinstance(lst_day_value, dict):\n",
        "        val = lst_day_value.get('LST_Day_1km', None)\n",
        "        if val is None: return None\n",
        "    else:\n",
        "        val = lst_day_value\n",
        "\n",
        "    temp_kelvin = val * 0.02 # MODIS LST scaling to Kelvin\n",
        "    stefan_boltzmann = 5.67e-8  # W/(m^2 K^4)\n",
        "    # This is a simplified Stefan-Boltzmann law application, assuming emissivity = 1\n",
        "    # The reference notebook had this calculation. True GHI/DNI would come from datasets like ERA5 or specific solar datasets.\n",
        "    return stefan_boltzmann * (temp_kelvin ** 4)\n",
        "\n",
        "def extract_radiacion_solar(region_geojson, start_date_str, end_date_str, frequency, output_dir, scale=1000):\n",
        "    return extract_gee_time_series(\n",
        "        variable_name=\"Radiacion_Solar_Approximated_from_LST\",\n",
        "        region_geojson=region_geojson,\n",
        "        start_date_str=start_date_str,\n",
        "        end_date_str=end_date_str,\n",
        "        frequency=frequency,\n",
        "        gee_dataset_id='MODIS/061/MOD11A1', # Using LST dataset as per reference\n",
        "        gee_band_name='LST_Day_1km',       # Using LST Day band\n",
        "        scale=scale,\n",
        "        output_dir=output_dir,\n",
        "        post_process_function=post_process_solar_radiation,\n",
        "        nan_value=np.nan\n",
        "    )\n",
        "\n",
        "# Temperature\n",
        "def extract_temperatura_dia(region_geojson, start_date_str, end_date_str, frequency, output_dir, scale=1000):\n",
        "    return extract_gee_time_series(\n",
        "        variable_name=\"Temperatura_Dia_Celsius\",\n",
        "        region_geojson=region_geojson,\n",
        "        start_date_str=start_date_str,\n",
        "        end_date_str=end_date_str,\n",
        "        frequency=frequency,\n",
        "        gee_dataset_id='MODIS/061/MOD11A1', # As per reference notebook\n",
        "        gee_band_name='LST_Day_1km',\n",
        "        scale=scale,\n",
        "        output_dir=output_dir,\n",
        "        post_process_function=lambda x: convert_modis_lst_to_celsius(x.get('LST_Day_1km') if isinstance(x, dict) else x) if x is not None else None, # LST to Celsius\n",
        "        nan_value=np.nan\n",
        "    )\n",
        "\n",
        "def extract_temperatura_noche(region_geojson, start_date_str, end_date_str, frequency, output_dir, scale=1000):\n",
        "    return extract_gee_time_series(\n",
        "        variable_name=\"Temperatura_Noche_Celsius\",\n",
        "        region_geojson=region_geojson,\n",
        "        start_date_str=start_date_str,\n",
        "        end_date_str=end_date_str,\n",
        "        frequency=frequency,\n",
        "        gee_dataset_id='MODIS/061/MOD11A1', # As per reference notebook\n",
        "        gee_band_name='LST_Night_1km',\n",
        "        scale=scale,\n",
        "        output_dir=output_dir,\n",
        "        post_process_function=lambda x: convert_modis_lst_to_celsius(x.get('LST_Night_1km') if isinstance(x, dict) else x) if x is not None else None, # LST to Celsius\n",
        "        nan_value=np.nan\n",
        "    )"
      ],
      "id": "XHlRJn9Y3cAd"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "kd8QV5hk3cAe"
      },
      "outputs": [],
      "source": [
        "# --- Wind Speed and Direction ---\n",
        "def post_process_wind_data(wind_components):\n",
        "    \"\"\"Calculates wind speed and direction from u and v components.\"\"\"\n",
        "    u = wind_components.get('u_component_of_wind_10m')\n",
        "    v = wind_components.get('v_component_of_wind_10m')\n",
        "\n",
        "    if u is None or v is None:\n",
        "        return {'Viento_Velocidad': np.nan, 'Viento_Direccion': np.nan}\n",
        "\n",
        "    speed = math.sqrt(u**2 + v**2)\n",
        "    # Wind direction: meteorological convention (degrees from North, clockwise)\n",
        "    # atan2(u,v) gives angle w.r.t positive y-axis (North). Convert to degrees.\n",
        "    # Then adjust to be 0-360.\n",
        "    # direction_rad = math.atan2(u, v) # u is eastward, v is northward\n",
        "    # direction_deg = math.degrees(direction_rad)\n",
        "    # direction_met = (270 - direction_deg) % 360 # As in reference notebook\n",
        "    # A common formula for meteorological wind direction from u,v components:\n",
        "    direction_met = (180 / math.pi) * math.atan2(-u, -v) + 180\n",
        "    direction_met = direction_met % 360 # Ensure it's within 0-360\n",
        "\n",
        "    return {'Viento_Velocidad': speed, 'Viento_Direccion': direction_met}\n",
        "\n",
        "def extract_viento(region_geojson, start_date_str, end_date_str, frequency, output_dir, scale=10000):\n",
        "    # Note: ERA5 Land is hourly. If 'daily', 'monthly', 'yearly' frequency is requested,\n",
        "    # the generic function will average the hourly u/v components first, then calculate speed/direction.\n",
        "    return extract_gee_time_series(\n",
        "        variable_name=\"Viento\", # Base name, will be expanded by post_process_wind_data keys\n",
        "        region_geojson=region_geojson,\n",
        "        start_date_str=start_date_str,\n",
        "        end_date_str=end_date_str,\n",
        "        frequency=frequency,\n",
        "        gee_dataset_id='ECMWF/ERA5_LAND/HOURLY', # As per reference notebook\n",
        "        gee_band_name=['u_component_of_wind_10m', 'v_component_of_wind_10m'],\n",
        "        scale=scale,\n",
        "        output_dir=output_dir,\n",
        "        post_process_function=post_process_wind_data,\n",
        "        nan_value=np.nan\n",
        "    )\n",
        "\n",
        "# --- Topography (Elevation, Slope, Aspect) ---\n",
        "# These are static, so they don't depend on date range or frequency in the same way.\n",
        "# The function will calculate mean values over the region for a single point in time (the SRTM image is static).\n",
        "def extract_topography(region_geojson, output_dir, scale=30, variable_name_prefix=\"Topografia_\"):\n",
        "    \"\"\"\n",
        "    Extracts mean Elevation, Slope, and Aspect for a region and saves to a CSV.\n",
        "    Output CSV will have one row with columns: 'Elevation', 'Slope', 'Aspect'.\n",
        "    \"\"\"\n",
        "    ee_region = ee.Geometry(region_geojson)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        srtm = ee.Image('USGS/SRTMGL1_003') # SRTM is a single image\n",
        "        elevation = srtm.select('elevation')\n",
        "        slope = ee.Terrain.slope(elevation)\n",
        "        aspect = ee.Terrain.aspect(elevation)\n",
        "\n",
        "        topography_image = ee.Image.cat([elevation, slope, aspect]).rename(['Elevacion', 'Pendiente', 'Aspecto'])\n",
        "\n",
        "        reduction = topography_image.reduceRegion(\n",
        "            reducer=ee.Reducer.mean(),\n",
        "            geometry=ee_region,\n",
        "            scale=scale,\n",
        "            maxPixels=1e10,\n",
        "            bestEffort=True,\n",
        "            tileScale=0.1\n",
        "        )\n",
        "\n",
        "        # GetInfo once\n",
        "        reduced_data = reduction.getInfo()\n",
        "\n",
        "        # Handle cases where a key might be missing (though unlikely for these specific bands from SRTM)\n",
        "        data_for_df = {\n",
        "            'Elevacion': reduced_data.get('Elevacion', np.nan),\n",
        "            'Pendiente': reduced_data.get('Pendiente', np.nan),\n",
        "            'Aspecto': reduced_data.get('Aspecto', np.nan)\n",
        "        }\n",
        "\n",
        "        df = pd.DataFrame([data_for_df])\n",
        "\n",
        "        # Define a simple, non-temporal filename\n",
        "        csv_filename = f\"{variable_name_prefix}mean_values.csv\"\n",
        "        csv_path = os.path.join(output_dir, csv_filename)\n",
        "        df.to_csv(csv_path, index=False, na_rep=str(np.nan))\n",
        "\n",
        "        print(f\"Successfully saved topography data to {csv_path}\")\n",
        "        return csv_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting topography data: {e}\")\n",
        "        # Create a CSV with NaNs if there's an error\n",
        "        df = pd.DataFrame([{'Elevacion': np.nan, 'Pendiente': np.nan, 'Aspecto': np.nan}])\n",
        "        csv_filename = f\"{variable_name_prefix}mean_values_error.csv\"\n",
        "        csv_path = os.path.join(output_dir, csv_filename)\n",
        "        df.to_csv(csv_path, index=False, na_rep=str(np.nan))\n",
        "        return csv_path\n",
        "\n",
        "\n",
        "# --- Land Cover (Principal Type and Percentage) ---\n",
        "# This is typically analyzed for a specific year or period, not as a continuous time series like temperature.\n",
        "# The function will find the dominant land cover type and its percentage for a given year.\n",
        "def extract_land_cover(region_geojson, year_str, output_dir, scale=500, variable_name_prefix=\"Cobertura_\"):\n",
        "    \"\"\"\n",
        "    Extracts the principal land cover type and its percentage for a region and year.\n",
        "    Saves to a CSV with columns: 'Year', 'Cobertura_Terrestre_Principal', 'Cobertura_Terrestre_Porcentaje'.\n",
        "    Uses MODIS MCD12Q1 dataset.\n",
        "    \"\"\"\n",
        "    ee_region = ee.Geometry(region_geojson)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # MODIS Land Cover Type 1 (IGBP classification)\n",
        "        # MCD12Q1 provides yearly data. Filter for the specific year.\n",
        "        start_date = f\"{year_str}-01-01\"\n",
        "        end_date = f\"{year_str}-12-31\" # End of the year\n",
        "\n",
        "        land_cover_collection = ee.ImageCollection('MODIS/061/MCD12Q1') \\\n",
        "                                  .filterDate(start_date, end_date) \\\n",
        "                                  .select('LC_Type1') # IGBP classification band\n",
        "\n",
        "        # Get the image for the year (should be one)\n",
        "        land_cover_image = land_cover_collection.first()\n",
        "\n",
        "        # A robust check to see if the image is valid and has bands\n",
        "        # Attempt to get band names; if it fails or is empty, image is likely invalid/empty\n",
        "        valid_image = False\n",
        "        try:\n",
        "            if land_cover_image.bandNames().size().getInfo() > 0:\n",
        "                valid_image = True\n",
        "        except Exception: # Handles cases where land_cover_image might be null or not a proper image\n",
        "            valid_image = False\n",
        "\n",
        "        if not valid_image:\n",
        "             print(f\"No MODIS land cover data found or image is invalid for year {year_str}.\")\n",
        "             data_for_df = {'Year': year_str, 'Cobertura_Terrestre_Principal': np.nan, 'Cobertura_Terrestre_Porcentaje': np.nan}\n",
        "        else:\n",
        "            # Calculate frequency histogram of land cover types in the region\n",
        "            histogram = land_cover_image.reduceRegion(\n",
        "                reducer=ee.Reducer.frequencyHistogram(),\n",
        "                geometry=ee_region,\n",
        "                scale=scale,\n",
        "                maxPixels=1e10,\n",
        "                bestEffort=True,\n",
        "                tileScale=0.1\n",
        "            ).get('LC_Type1') # Get the histogram for the band\n",
        "\n",
        "            histogram_info = histogram.getInfo() # This can be slow for very large regions\n",
        "\n",
        "            if not histogram_info: # Check if histogram is empty\n",
        "                print(f\"Land cover histogram is empty for year {year_str} in the region.\")\n",
        "                data_for_df = {'Year': year_str, 'Cobertura_Terrestre_Principal': np.nan, 'Cobertura_Terrestre_Porcentaje': np.nan}\n",
        "            else:\n",
        "                # Convert keys (class IDs) to integers and find the principal class\n",
        "                class_counts = {int(k): v for k, v in histogram_info.items()}\n",
        "                total_pixels = sum(class_counts.values())\n",
        "\n",
        "                if total_pixels == 0:\n",
        "                    principal_class_id = np.nan\n",
        "                    percentage = np.nan\n",
        "                else:\n",
        "                    principal_class_id = max(class_counts, key=class_counts.get)\n",
        "                    percentage = (class_counts[principal_class_id] / total_pixels) * 100\n",
        "\n",
        "                data_for_df = {\n",
        "                    'Year': year_str,\n",
        "                    'Cobertura_Terrestre_Principal': principal_class_id,\n",
        "                    'Cobertura_Terrestre_Porcentaje': percentage\n",
        "                }\n",
        "\n",
        "        df = pd.DataFrame([data_for_df])\n",
        "        csv_filename = f\"{variable_name_prefix}{year_str}.csv\"\n",
        "        csv_path = os.path.join(output_dir, csv_filename)\n",
        "        df.to_csv(csv_path, index=False, na_rep=str(np.nan))\n",
        "\n",
        "        print(f\"Successfully saved land cover data for {year_str} to {csv_path}\")\n",
        "        return csv_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting land cover data for {year_str}: {e}\")\n",
        "        df = pd.DataFrame([{'Year': year_str, 'Cobertura_Terrestre_Principal': np.nan, 'Cobertura_Terrestre_Porcentaje': np.nan}])\n",
        "        csv_filename = f\"{variable_name_prefix}{year_str}_error.csv\"\n",
        "        csv_path = os.path.join(output_dir, csv_filename)\n",
        "        df.to_csv(csv_path, index=False, na_rep=str(np.nan))\n",
        "        return csv_path\n"
      ],
      "id": "kd8QV5hk3cAe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7KU5Tzw3cAe"
      },
      "source": [
        "---\n",
        "## Example Usage of Extraction Functions\n",
        "\n",
        "Below are examples of how to use the implemented functions.\n",
        "- You'll need to define your `region_of_interest` (as a GeoJSON-like Python dictionary).\n",
        "- Specify your desired `start_date`, `end_date`, `year_for_landcover`, and `output_directory`.\n",
        "- Uncomment the function calls you wish to run.\n",
        "- Ensure the `output_directory` exists or the functions will create it."
      ],
      "id": "A7KU5Tzw3cAe"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import re\n",
        "\n",
        "def dms_to_decimal(coord_str):\n",
        "    # First, try to parse as a decimal degree with optional direction\n",
        "    decimal_pattern = re.compile(\n",
        "        r'^\\s*([+-]?[\\d.]+)\\s*([NSEW]?)\\s*$',\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "    decimal_match = decimal_pattern.match(coord_str.strip())\n",
        "    if decimal_match:\n",
        "        number_str, direction = decimal_match.groups()\n",
        "        try:\n",
        "            number = float(number_str)\n",
        "        except ValueError:\n",
        "            pass  # Not a valid decimal, proceed to check DMS\n",
        "        else:\n",
        "            if direction:\n",
        "                direction = direction.upper()\n",
        "                decimal = abs(number)\n",
        "                if direction in ['S', 'W']:\n",
        "                    decimal *= -1\n",
        "                return decimal\n",
        "            else:\n",
        "                return number\n",
        "\n",
        "    # If not a decimal, attempt to parse as DMS\n",
        "    dms_pattern = re.compile(\n",
        "        r'''\\s*([+-]?\\d+)\\s*°\\s*([+-]?\\d+)\\s*'\\s*([+-]?\\d+\\.?\\d*)\\s*\"*\\s*([NSEW]?)\\s*''',\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "    dms_match = dms_pattern.match(coord_str.strip())\n",
        "    if not dms_match:\n",
        "        raise ValueError(f\"Invalid coordinate format: {coord_str}\")\n",
        "\n",
        "    degrees, minutes, seconds, direction = dms_match.groups()\n",
        "    degrees = float(degrees)\n",
        "    minutes = float(minutes)\n",
        "    seconds = float(seconds)\n",
        "    decimal = degrees + minutes / 60 + seconds / 3600\n",
        "\n",
        "    if direction.upper() in ['S', 'W']:\n",
        "        decimal *= -1\n",
        "\n",
        "    return decimal\n",
        "\n",
        "def get_point(center_lat, center_lon, distance_km, bearing_deg):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    delta = distance_km / R  # Angular distance in radians\n",
        "\n",
        "    lat1 = math.radians(center_lat)\n",
        "    lon1 = math.radians(center_lon)\n",
        "    theta = math.radians(bearing_deg)\n",
        "\n",
        "    lat2 = math.asin(\n",
        "        math.sin(lat1) * math.cos(delta) +\n",
        "        math.cos(lat1) * math.sin(delta) * math.cos(theta))\n",
        "    lon2 = lon1 + math.atan2(\n",
        "        math.sin(theta) * math.sin(delta) * math.cos(lat1),\n",
        "        math.cos(delta) - math.sin(lat1) * math.sin(lat2))\n",
        "\n",
        "    lon2 = (lon2 + 3 * math.pi) % (2 * math.pi) - math.pi\n",
        "\n",
        "    return (math.degrees(lat2), math.degrees(lon2))\n",
        "\n",
        "def generate_polygon_vertices(lat_input, lon_input, num_points, distance_km):\n",
        "    # Convert inputs to decimal degrees\n",
        "    try:\n",
        "        center_lat = dms_to_decimal(lat_input)\n",
        "        center_lon = dms_to_decimal(lon_input)\n",
        "    except ValueError as e:\n",
        "        raise ValueError(f\"Invalid coordinate format: {e}\")\n",
        "\n",
        "    vertices = []\n",
        "    for i in range(num_points):\n",
        "        bearing = (i * 360.0) / num_points\n",
        "        lat, lon = get_point(center_lat, center_lon, distance_km, bearing)\n",
        "        vertices.append((lat, lon))\n",
        "    return vertices"
      ],
      "metadata": {
        "id": "lA4EDcvK4RtT"
      },
      "id": "lA4EDcvK4RtT",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_vertices_to_geojson(vertices):\n",
        "    \"\"\"\n",
        "    Convert polygon vertices to a GeoJSON-like Polygon dictionary.\n",
        "\n",
        "    Args:\n",
        "        vertices (list): List of (latitude, longitude) tuples\n",
        "\n",
        "    Returns:\n",
        "        dict: GeoJSON-like Polygon dictionary\n",
        "    \"\"\"\n",
        "    # Convert (latitude, longitude) to (longitude, latitude) for GeoJSON\n",
        "    coordinates = [[lon, lat] for lat, lon in vertices]\n",
        "\n",
        "    # Ensure the polygon is closed by appending the first coordinate if necessary\n",
        "    if coordinates:\n",
        "        if coordinates[0] != coordinates[-1]:\n",
        "            coordinates.append(coordinates[0])\n",
        "\n",
        "    return {\n",
        "        'type': 'Polygon',\n",
        "        'coordinates': [coordinates]\n",
        "    }"
      ],
      "metadata": {
        "id": "M99R4ClsB5O3"
      },
      "id": "M99R4ClsB5O3",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "47a1iGmpF6bx"
      },
      "id": "47a1iGmpF6bx"
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "from folium import Figure\n",
        "from html2image import Html2Image\n",
        "\n",
        "def visualize_with_folium(vertices, center_lat=None, center_lon=None,\n",
        "                          zoom_start=12, save_path=\"map.html\"):\n",
        "    \"\"\"\n",
        "    Creates a Folium map with a polygon and saves it as an HTML file.\n",
        "\n",
        "    :param vertices: List of [lat, lon] pairs defining the polygon\n",
        "    :param center_lat: Center latitude (optional)\n",
        "    :param center_lon: Center longitude (optional)\n",
        "    :param zoom_start: Initial zoom level\n",
        "    :param save_path: Path to save the HTML file\n",
        "    :return: Folium Map object\n",
        "    \"\"\"\n",
        "    # Calculate center if not provided\n",
        "    if center_lat is None or center_lon is None:\n",
        "        center_lat = sum(v[0] for v in vertices) / len(vertices)\n",
        "        center_lon = sum(v[1] for v in vertices) / len(vertices)\n",
        "\n",
        "    # Create a Figure container\n",
        "    figure = Figure(width=800, height=600)\n",
        "\n",
        "    # Create base map inside the figure\n",
        "    map_obj = folium.Map(\n",
        "        location=[center_lat, center_lon],\n",
        "        zoom_start=zoom_start,\n",
        "        tiles='Esri.WorldImagery'\n",
        "    )\n",
        "    figure.add_child(map_obj)\n",
        "\n",
        "    # Add elements\n",
        "    folium.Marker(\n",
        "        [center_lat, center_lon],\n",
        "        popup=\"Center\",\n",
        "        icon=folium.Icon(color='red')\n",
        "    ).add_to(map_obj)\n",
        "\n",
        "    folium.Polygon(\n",
        "        vertices,\n",
        "        color='#ff0000',\n",
        "        fill=True,\n",
        "        fill_opacity=0.2\n",
        "    ).add_to(map_obj)\n",
        "\n",
        "    # Save HTML\n",
        "    map_obj.save(save_path)\n",
        "\n",
        "    return map_obj"
      ],
      "metadata": {
        "id": "BeSEYgLu_ULD"
      },
      "id": "BeSEYgLu_ULD",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy coordinates from google earth"
      ],
      "metadata": {
        "id": "AKjR-MQ8HoAm"
      },
      "id": "AKjR-MQ8HoAm"
    },
    {
      "cell_type": "code",
      "source": [
        "from mmap import MAP_ANON\n",
        "# Generate vertices\n",
        "vertices = generate_polygon_vertices(\n",
        "    lat_input= LATITUDE,\n",
        "    lon_input= LONGITUDE,\n",
        "    num_points= POLIGON_VERTICES,\n",
        "    distance_km= RADIO\n",
        ")\n",
        "\n",
        "# Define paths\n",
        "map_name = f'{LOCATION_NAME}_map_{DESCRIPCION}'\n",
        "\n",
        "# Define the output directory for CSV files\n",
        "output_directory = os.path.join(os.getcwd(),map_name)\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "png_file_path = os.path.join(output_directory, f\"{map_name}.png\")\n",
        "html_map_path = os.path.join(output_directory,f\"{map_name}.html\")\n",
        "\n",
        "\n",
        "# Create visualization and save HTML\n",
        "map_obj = visualize_with_folium(\n",
        "    vertices=vertices,\n",
        "    save_path=html_map_path,\n",
        "    zoom_start=12\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "RFUzsxWT_XcL"
      },
      "id": "RFUzsxWT_XcL",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "import os\n",
        "import nest_asyncio # Import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to allow asyncio to run inside a loop (like Jupyter)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def take_screenshot(html_path, output_png_path, width=800, height=600):\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch() # Or firefox, webkit\n",
        "\n",
        "        # --- Add this line to create a BrowserContext ---\n",
        "        context = await browser.new_context(viewport={'width': width, 'height': height})\n",
        "        # --- End of added line ---\n",
        "\n",
        "        # --- Modify this line to create the page from the context ---\n",
        "        page = await context.new_page() # Create the page within the context\n",
        "        # --- End of modified line ---\n",
        "\n",
        "        # Construct file URL\n",
        "        file_url = f'file://{os.path.abspath(html_path)}'\n",
        "        print(f\"Attempting to load HTML file from: {file_url}\") # Debugging print\n",
        "\n",
        "        try:\n",
        "            await page.goto(file_url)\n",
        "            print(f\"Successfully loaded {html_path}\")\n",
        "            await page.screenshot(path=output_png_path)\n",
        "            print(f\"Screenshot saved to: {output_png_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during screenshot: {e}\")\n",
        "        finally:\n",
        "            # --- Close the context and then the browser ---\n",
        "            await context.close() # Close the context\n",
        "            await browser.close() # Close the browser\n",
        "            # --- End of closing lines ---\n",
        "            print(\"Browser and context closed.\")\n"
      ],
      "metadata": {
        "id": "zHrVYkkHQblv"
      },
      "id": "zHrVYkkHQblv",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now run the async function\n",
        "# You can use asyncio.run() after applying nest_asyncio, or the %autoawait magic\n",
        "\n",
        "# Using asyncio.run()\n",
        "try:\n",
        "    asyncio.run(take_screenshot(html_map_path, png_file_path, width=800, height=600))\n",
        "except Exception as e:\n",
        "     print(f\"Error running asyncio task: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5Ba4WWxR15n",
        "outputId": "bbed9d80-eb63-4822-dd7b-a5c5585458a1"
      },
      "id": "m5Ba4WWxR15n",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load HTML file from: file:///content/BELA BELA_map_Zona sin vegetacion/BELA BELA_map_Zona sin vegetacion.html\n",
            "Successfully loaded /content/BELA BELA_map_Zona sin vegetacion/BELA BELA_map_Zona sin vegetacion.html\n",
            "Screenshot saved to: /content/BELA BELA_map_Zona sin vegetacion/BELA BELA_map_Zona sin vegetacion.png\n",
            "Browser and context closed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKI8nHbD3cAe",
        "outputId": "20868bdc-fcc0-4498-95df-5918c5b8eb2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing period 2023-01-01\n",
            "Processing period 2023-01-02\n",
            "Processing period 2023-01-03\n",
            "Processing period 2023-01-04\n",
            "Processing period 2023-01-05\n",
            "Processing period 2023-01-06\n",
            "Processing period 2023-01-07\n",
            "Processing period 2023-01-08\n",
            "Processing period 2023-01-09\n",
            "Processing period 2023-01-10\n",
            "Processing period 2023-01-11\n",
            "Processing period 2023-01-12\n",
            "Processing period 2023-01-13\n",
            "Processing period 2023-01-14\n",
            "Processing period 2023-01-15\n",
            "Processing period 2023-01-16\n",
            "Processing period 2023-01-17\n",
            "Processing period 2023-01-18\n",
            "Processing period 2023-01-19\n",
            "Processing period 2023-01-20\n",
            "Processing period 2023-01-21\n",
            "Processing period 2023-01-22\n",
            "Processing period 2023-01-23\n",
            "Processing period 2023-01-24\n",
            "Processing period 2023-01-25\n",
            "Processing period 2023-01-26\n",
            "Processing period 2023-01-27\n",
            "Processing period 2023-01-28\n",
            "Processing period 2023-01-29\n",
            "Processing period 2023-01-30\n",
            "Processing period 2023-01-31\n",
            "Processing period 2023-02-01\n",
            "Processing period 2023-02-02\n",
            "Processing period 2023-02-03\n",
            "Processing period 2023-02-04\n",
            "Processing period 2023-02-05\n",
            "Processing period 2023-02-06\n",
            "Processing period 2023-02-07\n",
            "Processing period 2023-02-08\n",
            "Processing period 2023-02-09\n",
            "Processing period 2023-02-10\n",
            "Processing period 2023-02-11\n",
            "Processing period 2023-02-12\n",
            "Processing period 2023-02-13\n",
            "Processing period 2023-02-14\n",
            "Processing period 2023-02-15\n",
            "Processing period 2023-02-16\n",
            "Processing period 2023-02-17\n",
            "Processing period 2023-02-18\n",
            "Processing period 2023-02-19\n",
            "Processing period 2023-02-20\n",
            "Processing period 2023-02-21\n",
            "Processing period 2023-02-22\n",
            "Processing period 2023-02-23\n",
            "Processing period 2023-02-24\n",
            "Processing period 2023-02-25\n",
            "Processing period 2023-02-26\n",
            "Processing period 2023-02-27\n",
            "Processing period 2023-02-28\n",
            "Processing period 2023-03-01\n",
            "Processing period 2023-03-02\n",
            "Processing period 2023-03-03\n",
            "Processing period 2023-03-04\n",
            "Processing period 2023-03-05\n",
            "Processing period 2023-03-06\n",
            "Processing period 2023-03-07\n",
            "Processing period 2023-03-08\n",
            "Processing period 2023-03-09\n",
            "Processing period 2023-03-10\n",
            "Processing period 2023-03-11\n",
            "Processing period 2023-03-12\n",
            "Processing period 2023-03-13\n",
            "Processing period 2023-03-14\n",
            "Processing period 2023-03-15\n",
            "Processing period 2023-03-16\n",
            "Processing period 2023-03-17\n",
            "Processing period 2023-03-18\n",
            "Processing period 2023-03-19\n",
            "Processing period 2023-03-20\n",
            "Processing period 2023-03-21\n",
            "Processing period 2023-03-22\n",
            "Processing period 2023-03-23\n",
            "Processing period 2023-03-24\n",
            "Processing period 2023-03-25\n",
            "Processing period 2023-03-26\n",
            "Processing period 2023-03-27\n",
            "Processing period 2023-03-28\n",
            "Processing period 2023-03-29\n",
            "Processing period 2023-03-30\n",
            "Processing period 2023-03-31\n",
            "Processing period 2023-04-01\n",
            "Processing period 2023-04-02\n",
            "Processing period 2023-04-03\n",
            "Processing period 2023-04-04\n",
            "Processing period 2023-04-05\n",
            "Processing period 2023-04-06\n",
            "Processing period 2023-04-07\n",
            "Processing period 2023-04-08\n",
            "Processing period 2023-04-09\n",
            "Processing period 2023-04-10\n",
            "Processing period 2023-04-11\n",
            "Processing period 2023-04-12\n",
            "Processing period 2023-04-13\n",
            "Processing period 2023-04-14\n",
            "Processing period 2023-04-15\n",
            "Processing period 2023-04-16\n",
            "Processing period 2023-04-17\n",
            "Processing period 2023-04-18\n",
            "Processing period 2023-04-19\n",
            "Processing period 2023-04-20\n",
            "Processing period 2023-04-21\n",
            "Processing period 2023-04-22\n",
            "Processing period 2023-04-23\n",
            "Processing period 2023-04-24\n",
            "Processing period 2023-04-25\n",
            "Processing period 2023-04-26\n",
            "Processing period 2023-04-27\n",
            "Processing period 2023-04-28\n",
            "Processing period 2023-04-29\n",
            "Processing period 2023-04-30\n",
            "Processing period 2023-05-01\n",
            "Processing period 2023-05-02\n",
            "Processing period 2023-05-03\n",
            "Processing period 2023-05-04\n",
            "Processing period 2023-05-05\n",
            "Processing period 2023-05-06\n",
            "Processing period 2023-05-07\n",
            "Processing period 2023-05-08\n",
            "Processing period 2023-05-09\n",
            "Processing period 2023-05-10\n",
            "Processing period 2023-05-11\n",
            "Processing period 2023-05-12\n",
            "Processing period 2023-05-13\n",
            "Processing period 2023-05-14\n",
            "Processing period 2023-05-15\n",
            "Processing period 2023-05-16\n",
            "Processing period 2023-05-17\n",
            "Processing period 2023-05-18\n",
            "Processing period 2023-05-19\n",
            "Processing period 2023-05-20\n",
            "Processing period 2023-05-21\n",
            "Processing period 2023-05-22\n",
            "Processing period 2023-05-23\n",
            "Processing period 2023-05-24\n",
            "Processing period 2023-05-25\n",
            "Processing period 2023-05-26\n",
            "Processing period 2023-05-27\n",
            "Processing period 2023-05-28\n",
            "Processing period 2023-05-29\n",
            "Processing period 2023-05-30\n",
            "Processing period 2023-05-31\n",
            "Processing period 2023-06-01\n",
            "Processing period 2023-06-02\n",
            "Processing period 2023-06-03\n",
            "Processing period 2023-06-04\n",
            "Processing period 2023-06-05\n",
            "Processing period 2023-06-06\n",
            "Processing period 2023-06-07\n",
            "Processing period 2023-06-08\n",
            "Processing period 2023-06-09\n",
            "Processing period 2023-06-10\n",
            "Processing period 2023-06-11\n",
            "Processing period 2023-06-12\n",
            "Processing period 2023-06-13\n",
            "Processing period 2023-06-14\n",
            "Processing period 2023-06-15\n",
            "Processing period 2023-06-16\n",
            "Processing period 2023-06-17\n",
            "Processing period 2023-06-18\n",
            "Processing period 2023-06-19\n",
            "Processing period 2023-06-20\n",
            "Processing period 2023-06-21\n",
            "Processing period 2023-06-22\n",
            "Processing period 2023-06-23\n",
            "Processing period 2023-06-24\n",
            "Processing period 2023-06-25\n",
            "Processing period 2023-06-26\n",
            "Processing period 2023-06-27\n",
            "Processing period 2023-06-28\n",
            "Processing period 2023-06-29\n",
            "Processing period 2023-06-30\n",
            "Processing period 2023-07-01\n",
            "Processing period 2023-07-02\n",
            "Processing period 2023-07-03\n",
            "Processing period 2023-07-04\n",
            "Processing period 2023-07-05\n",
            "Processing period 2023-07-06\n",
            "Processing period 2023-07-07\n",
            "Processing period 2023-07-08\n",
            "Processing period 2023-07-09\n",
            "Processing period 2023-07-10\n",
            "Processing period 2023-07-11\n",
            "Processing period 2023-07-12\n",
            "Processing period 2023-07-13\n",
            "Processing period 2023-07-14\n",
            "Processing period 2023-07-15\n",
            "Processing period 2023-07-16\n",
            "Processing period 2023-07-17\n",
            "Processing period 2023-07-18\n",
            "Processing period 2023-07-19\n",
            "Processing period 2023-07-20\n",
            "Processing period 2023-07-21\n",
            "Processing period 2023-07-22\n",
            "Processing period 2023-07-23\n",
            "Processing period 2023-07-24\n",
            "Processing period 2023-07-25\n",
            "Processing period 2023-07-26\n",
            "Processing period 2023-07-27\n",
            "Processing period 2023-07-28\n",
            "Processing period 2023-07-29\n",
            "Processing period 2023-07-30\n",
            "Processing period 2023-07-31\n",
            "Processing period 2023-08-01\n",
            "Processing period 2023-08-02\n",
            "Processing period 2023-08-03\n",
            "Processing period 2023-08-04\n",
            "Processing period 2023-08-05\n",
            "Processing period 2023-08-06\n",
            "Processing period 2023-08-07\n",
            "Processing period 2023-08-08\n",
            "Processing period 2023-08-09\n",
            "Processing period 2023-08-10\n",
            "Processing period 2023-08-11\n",
            "Processing period 2023-08-12\n",
            "Processing period 2023-08-13\n",
            "Processing period 2023-08-14\n",
            "Processing period 2023-08-15\n",
            "Processing period 2023-08-16\n",
            "Processing period 2023-08-17\n",
            "Processing period 2023-08-18\n",
            "Processing period 2023-08-19\n",
            "Processing period 2023-08-20\n",
            "Processing period 2023-08-21\n",
            "Processing period 2023-08-22\n",
            "Processing period 2023-08-23\n",
            "Processing period 2023-08-24\n",
            "Processing period 2023-08-25\n",
            "Processing period 2023-08-26\n",
            "Processing period 2023-08-27\n",
            "Processing period 2023-08-28\n",
            "Processing period 2023-08-29\n",
            "Processing period 2023-08-30\n",
            "Processing period 2023-08-31\n",
            "Processing period 2023-09-01\n",
            "Processing period 2023-09-02\n",
            "Processing period 2023-09-03\n",
            "Processing period 2023-09-04\n",
            "Processing period 2023-09-05\n",
            "Processing period 2023-09-06\n",
            "Processing period 2023-09-07\n",
            "Processing period 2023-09-08\n",
            "Processing period 2023-09-09\n",
            "Processing period 2023-09-10\n",
            "Processing period 2023-09-11\n",
            "Processing period 2023-09-12\n",
            "Processing period 2023-09-13\n",
            "Processing period 2023-09-14\n",
            "Processing period 2023-09-15\n",
            "Processing period 2023-09-16\n",
            "Processing period 2023-09-17\n",
            "Processing period 2023-09-18\n",
            "Processing period 2023-09-19\n",
            "Processing period 2023-09-20\n",
            "Processing period 2023-09-21\n",
            "Processing period 2023-09-22\n",
            "Processing period 2023-09-23\n",
            "Processing period 2023-09-24\n",
            "Processing period 2023-09-25\n",
            "Processing period 2023-09-26\n",
            "Processing period 2023-09-27\n",
            "Processing period 2023-09-28\n",
            "Processing period 2023-09-29\n",
            "Processing period 2023-09-30\n",
            "Processing period 2023-10-01\n",
            "Processing period 2023-10-02\n",
            "Processing period 2023-10-03\n",
            "Processing period 2023-10-04\n",
            "Processing period 2023-10-05\n",
            "Processing period 2023-10-06\n",
            "Processing period 2023-10-07\n",
            "Processing period 2023-10-08\n",
            "Processing period 2023-10-09\n",
            "Processing period 2023-10-10\n",
            "Processing period 2023-10-11\n",
            "Processing period 2023-10-12\n",
            "Processing period 2023-10-13\n",
            "Processing period 2023-10-14\n",
            "Processing period 2023-10-15\n",
            "Processing period 2023-10-16\n",
            "Processing period 2023-10-17\n",
            "Processing period 2023-10-18\n",
            "Processing period 2023-10-19\n",
            "Processing period 2023-10-20\n",
            "Processing period 2023-10-21\n",
            "Processing period 2023-10-22\n",
            "Processing period 2023-10-23\n",
            "Processing period 2023-10-24\n",
            "Processing period 2023-10-25\n",
            "Processing period 2023-10-26\n",
            "Processing period 2023-10-27\n",
            "Processing period 2023-10-28\n",
            "Processing period 2023-10-29\n",
            "Processing period 2023-10-30\n",
            "Processing period 2023-10-31\n",
            "Processing period 2023-11-01\n",
            "Processing period 2023-11-02\n",
            "Processing period 2023-11-03\n",
            "Processing period 2023-11-04\n",
            "Processing period 2023-11-05\n",
            "Processing period 2023-11-06\n",
            "Processing period 2023-11-07\n",
            "Processing period 2023-11-08\n",
            "Processing period 2023-11-09\n",
            "Processing period 2023-11-10\n",
            "Processing period 2023-11-11\n",
            "Processing period 2023-11-12\n",
            "Processing period 2023-11-13\n",
            "Processing period 2023-11-14\n",
            "Processing period 2023-11-15\n",
            "Processing period 2023-11-16\n",
            "Processing period 2023-11-17\n",
            "Processing period 2023-11-18\n",
            "Processing period 2023-11-19\n",
            "Processing period 2023-11-20\n",
            "Processing period 2023-11-21\n",
            "Processing period 2023-11-22\n",
            "Processing period 2023-11-23\n",
            "Processing period 2023-11-24\n",
            "Processing period 2023-11-25\n",
            "Processing period 2023-11-26\n",
            "Processing period 2023-11-27\n",
            "Processing period 2023-11-28\n",
            "Processing period 2023-11-29\n",
            "Processing period 2023-11-30\n",
            "Successfully saved data for Albedo_BSA to /content/BELA BELA_map_Zona sin vegetacion/Albedo_BSA_daily_20230101_20231130.csv\n",
            "--- Extracting Albedo WSA (daily) ---\n",
            "Processing period 2023-01-01\n",
            "Processing period 2023-01-02\n",
            "Processing period 2023-01-03\n",
            "Processing period 2023-01-04\n",
            "Processing period 2023-01-05\n",
            "Processing period 2023-01-06\n",
            "Processing period 2023-01-07\n",
            "Processing period 2023-01-08\n",
            "Processing period 2023-01-09\n",
            "Processing period 2023-01-10\n",
            "Processing period 2023-01-11\n",
            "Processing period 2023-01-12\n",
            "Processing period 2023-01-13\n",
            "Processing period 2023-01-14\n",
            "Processing period 2023-01-15\n",
            "Processing period 2023-01-16\n",
            "Processing period 2023-01-17\n",
            "Processing period 2023-01-18\n",
            "Processing period 2023-01-19\n",
            "Processing period 2023-01-20\n",
            "Processing period 2023-01-21\n",
            "Processing period 2023-01-22\n",
            "Processing period 2023-01-23\n",
            "Processing period 2023-01-24\n",
            "Processing period 2023-01-25\n",
            "Processing period 2023-01-26\n",
            "Processing period 2023-01-27\n",
            "Processing period 2023-01-28\n",
            "Processing period 2023-01-29\n",
            "Processing period 2023-01-30\n",
            "Processing period 2023-01-31\n",
            "Processing period 2023-02-01\n",
            "Processing period 2023-02-02\n",
            "Processing period 2023-02-03\n",
            "Processing period 2023-02-04\n",
            "Processing period 2023-02-05\n",
            "Processing period 2023-02-06\n",
            "Processing period 2023-02-07\n",
            "Processing period 2023-02-08\n",
            "Processing period 2023-02-09\n",
            "Processing period 2023-02-10\n",
            "Processing period 2023-02-11\n",
            "Processing period 2023-02-12\n",
            "Processing period 2023-02-13\n",
            "Processing period 2023-02-14\n",
            "Processing period 2023-02-15\n",
            "Processing period 2023-02-16\n",
            "Processing period 2023-02-17\n",
            "Processing period 2023-02-18\n",
            "Processing period 2023-02-19\n",
            "Processing period 2023-02-20\n",
            "Processing period 2023-02-21\n",
            "Processing period 2023-02-22\n",
            "Processing period 2023-02-23\n",
            "Processing period 2023-02-24\n",
            "Processing period 2023-02-25\n",
            "Processing period 2023-02-26\n",
            "Processing period 2023-02-27\n",
            "Processing period 2023-02-28\n",
            "Processing period 2023-03-01\n",
            "Processing period 2023-03-02\n",
            "Processing period 2023-03-03\n",
            "Processing period 2023-03-04\n",
            "Processing period 2023-03-05\n",
            "Processing period 2023-03-06\n",
            "Processing period 2023-03-07\n",
            "Processing period 2023-03-08\n",
            "Processing period 2023-03-09\n",
            "Processing period 2023-03-10\n",
            "Processing period 2023-03-11\n",
            "Processing period 2023-03-12\n",
            "Processing period 2023-03-13\n",
            "Processing period 2023-03-14\n",
            "Processing period 2023-03-15\n",
            "Processing period 2023-03-16\n",
            "Processing period 2023-03-17\n",
            "Processing period 2023-03-18\n",
            "Processing period 2023-03-19\n",
            "Processing period 2023-03-20\n",
            "Processing period 2023-03-21\n",
            "Processing period 2023-03-22\n",
            "Processing period 2023-03-23\n",
            "Processing period 2023-03-24\n",
            "Processing period 2023-03-25\n",
            "Processing period 2023-03-26\n",
            "Processing period 2023-03-27\n",
            "Processing period 2023-03-28\n",
            "Processing period 2023-03-29\n",
            "Processing period 2023-03-30\n",
            "Processing period 2023-03-31\n",
            "Processing period 2023-04-01\n",
            "Processing period 2023-04-02\n",
            "Processing period 2023-04-03\n",
            "Processing period 2023-04-04\n",
            "Processing period 2023-04-05\n",
            "Processing period 2023-04-06\n",
            "Processing period 2023-04-07\n",
            "Processing period 2023-04-08\n",
            "Processing period 2023-04-09\n",
            "Processing period 2023-04-10\n",
            "Processing period 2023-04-11\n",
            "Processing period 2023-04-12\n",
            "Processing period 2023-04-13\n",
            "Processing period 2023-04-14\n",
            "Processing period 2023-04-15\n",
            "Processing period 2023-04-16\n",
            "Processing period 2023-04-17\n",
            "Processing period 2023-04-18\n",
            "Processing period 2023-04-19\n",
            "Processing period 2023-04-20\n",
            "Processing period 2023-04-21\n",
            "Processing period 2023-04-22\n",
            "Processing period 2023-04-23\n",
            "Processing period 2023-04-24\n",
            "Processing period 2023-04-25\n",
            "Processing period 2023-04-26\n",
            "Processing period 2023-04-27\n",
            "Processing period 2023-04-28\n",
            "Processing period 2023-04-29\n",
            "Processing period 2023-04-30\n",
            "Processing period 2023-05-01\n",
            "Processing period 2023-05-02\n",
            "Processing period 2023-05-03\n",
            "Processing period 2023-05-04\n",
            "Processing period 2023-05-05\n",
            "Processing period 2023-05-06\n",
            "Processing period 2023-05-07\n",
            "Processing period 2023-05-08\n",
            "Processing period 2023-05-09\n",
            "Processing period 2023-05-10\n",
            "Processing period 2023-05-11\n",
            "Processing period 2023-05-12\n",
            "Processing period 2023-05-13\n",
            "Processing period 2023-05-14\n",
            "Processing period 2023-05-15\n",
            "Processing period 2023-05-16\n",
            "Processing period 2023-05-17\n",
            "Processing period 2023-05-18\n",
            "Processing period 2023-05-19\n",
            "Processing period 2023-05-20\n",
            "Processing period 2023-05-21\n",
            "Processing period 2023-05-22\n",
            "Processing period 2023-05-23\n",
            "Processing period 2023-05-24\n",
            "Processing period 2023-05-25\n",
            "Processing period 2023-05-26\n",
            "Processing period 2023-05-27\n",
            "Processing period 2023-05-28\n",
            "Processing period 2023-05-29\n",
            "Processing period 2023-05-30\n",
            "Processing period 2023-05-31\n",
            "Processing period 2023-06-01\n",
            "Processing period 2023-06-02\n",
            "Processing period 2023-06-03\n",
            "Processing period 2023-06-04\n",
            "Processing period 2023-06-05\n",
            "Processing period 2023-06-06\n",
            "Processing period 2023-06-07\n",
            "Processing period 2023-06-08\n",
            "Processing period 2023-06-09\n",
            "Processing period 2023-06-10\n",
            "Processing period 2023-06-11\n",
            "Processing period 2023-06-12\n",
            "Processing period 2023-06-13\n",
            "Processing period 2023-06-14\n",
            "Processing period 2023-06-15\n",
            "Processing period 2023-06-16\n",
            "Processing period 2023-06-17\n",
            "Processing period 2023-06-18\n",
            "Processing period 2023-06-19\n",
            "Processing period 2023-06-20\n",
            "Processing period 2023-06-21\n",
            "Processing period 2023-06-22\n",
            "Processing period 2023-06-23\n",
            "Processing period 2023-06-24\n",
            "Processing period 2023-06-25\n",
            "Processing period 2023-06-26\n",
            "Processing period 2023-06-27\n",
            "Processing period 2023-06-28\n",
            "Processing period 2023-06-29\n",
            "Processing period 2023-06-30\n",
            "Processing period 2023-07-01\n",
            "Processing period 2023-07-02\n",
            "Processing period 2023-07-03\n",
            "Processing period 2023-07-04\n",
            "Processing period 2023-07-05\n",
            "Processing period 2023-07-06\n",
            "Processing period 2023-07-07\n",
            "Processing period 2023-07-08\n",
            "Processing period 2023-07-09\n",
            "Processing period 2023-07-10\n",
            "Processing period 2023-07-11\n",
            "Processing period 2023-07-12\n",
            "Processing period 2023-07-13\n",
            "Processing period 2023-07-14\n",
            "Processing period 2023-07-15\n",
            "Processing period 2023-07-16\n",
            "Processing period 2023-07-17\n",
            "Processing period 2023-07-18\n",
            "Processing period 2023-07-19\n",
            "Processing period 2023-07-20\n",
            "Processing period 2023-07-21\n",
            "Processing period 2023-07-22\n",
            "Processing period 2023-07-23\n",
            "Processing period 2023-07-24\n",
            "Processing period 2023-07-25\n",
            "Processing period 2023-07-26\n",
            "Processing period 2023-07-27\n",
            "Processing period 2023-07-28\n",
            "Processing period 2023-07-29\n",
            "Processing period 2023-07-30\n",
            "Processing period 2023-07-31\n",
            "Processing period 2023-08-01\n",
            "Processing period 2023-08-02\n",
            "Processing period 2023-08-03\n",
            "Processing period 2023-08-04\n",
            "Processing period 2023-08-05\n",
            "Processing period 2023-08-06\n",
            "Processing period 2023-08-07\n",
            "Processing period 2023-08-08\n",
            "Processing period 2023-08-09\n",
            "Processing period 2023-08-10\n",
            "Processing period 2023-08-11\n",
            "Processing period 2023-08-12\n",
            "Processing period 2023-08-13\n",
            "Processing period 2023-08-14\n",
            "Processing period 2023-08-15\n",
            "Processing period 2023-08-16\n",
            "Processing period 2023-08-17\n",
            "Processing period 2023-08-18\n",
            "Processing period 2023-08-19\n",
            "Processing period 2023-08-20\n",
            "Processing period 2023-08-21\n",
            "Processing period 2023-08-22\n",
            "Processing period 2023-08-23\n",
            "Processing period 2023-08-24\n",
            "Processing period 2023-08-25\n",
            "Processing period 2023-08-26\n",
            "Processing period 2023-08-27\n",
            "Processing period 2023-08-28\n",
            "Processing period 2023-08-29\n",
            "Processing period 2023-08-30\n",
            "Processing period 2023-08-31\n",
            "Processing period 2023-09-01\n",
            "Processing period 2023-09-02\n",
            "Processing period 2023-09-03\n",
            "Processing period 2023-09-04\n",
            "Processing period 2023-09-05\n",
            "Processing period 2023-09-06\n",
            "Processing period 2023-09-07\n",
            "Processing period 2023-09-08\n",
            "Processing period 2023-09-09\n",
            "Processing period 2023-09-10\n",
            "Processing period 2023-09-11\n",
            "Processing period 2023-09-12\n",
            "Processing period 2023-09-13\n",
            "Processing period 2023-09-14\n",
            "Processing period 2023-09-15\n",
            "Processing period 2023-09-16\n",
            "Processing period 2023-09-17\n",
            "Processing period 2023-09-18\n",
            "Processing period 2023-09-19\n",
            "Processing period 2023-09-20\n",
            "Processing period 2023-09-21\n",
            "Processing period 2023-09-22\n",
            "Processing period 2023-09-23\n",
            "Processing period 2023-09-24\n",
            "Processing period 2023-09-25\n",
            "Processing period 2023-09-26\n",
            "Processing period 2023-09-27\n",
            "Processing period 2023-09-28\n",
            "Processing period 2023-09-29\n",
            "Processing period 2023-09-30\n",
            "Processing period 2023-10-01\n",
            "Processing period 2023-10-02\n",
            "Processing period 2023-10-03\n",
            "Processing period 2023-10-04\n",
            "Processing period 2023-10-05\n",
            "Processing period 2023-10-06\n",
            "Processing period 2023-10-07\n",
            "Processing period 2023-10-08\n",
            "Processing period 2023-10-09\n",
            "Processing period 2023-10-10\n",
            "Processing period 2023-10-11\n",
            "Processing period 2023-10-12\n",
            "Processing period 2023-10-13\n",
            "Processing period 2023-10-14\n",
            "Processing period 2023-10-15\n",
            "Processing period 2023-10-16\n",
            "Processing period 2023-10-17\n",
            "Processing period 2023-10-18\n",
            "Processing period 2023-10-19\n",
            "Processing period 2023-10-20\n",
            "Processing period 2023-10-21\n",
            "Processing period 2023-10-22\n",
            "Processing period 2023-10-23\n",
            "Processing period 2023-10-24\n",
            "Processing period 2023-10-25\n",
            "Processing period 2023-10-26\n",
            "Processing period 2023-10-27\n",
            "Processing period 2023-10-28\n",
            "Processing period 2023-10-29\n",
            "Processing period 2023-10-30\n",
            "Processing period 2023-10-31\n",
            "Processing period 2023-11-01\n",
            "Processing period 2023-11-02\n",
            "Processing period 2023-11-03\n",
            "Processing period 2023-11-04\n",
            "Processing period 2023-11-05\n",
            "Processing period 2023-11-06\n",
            "Processing period 2023-11-07\n",
            "Processing period 2023-11-08\n",
            "Processing period 2023-11-09\n",
            "Processing period 2023-11-10\n",
            "Processing period 2023-11-11\n",
            "Processing period 2023-11-12\n",
            "Processing period 2023-11-13\n",
            "Processing period 2023-11-14\n",
            "Processing period 2023-11-15\n",
            "Processing period 2023-11-16\n",
            "Processing period 2023-11-17\n",
            "Processing period 2023-11-18\n",
            "Processing period 2023-11-19\n",
            "Processing period 2023-11-20\n",
            "Processing period 2023-11-21\n",
            "Processing period 2023-11-22\n",
            "Processing period 2023-11-23\n",
            "Processing period 2023-11-24\n",
            "Processing period 2023-11-25\n",
            "Processing period 2023-11-26\n",
            "Processing period 2023-11-27\n",
            "Processing period 2023-11-28\n",
            "Processing period 2023-11-29\n",
            "Processing period 2023-11-30\n",
            "Successfully saved data for Albedo_WSA to /content/BELA BELA_map_Zona sin vegetacion/Albedo_WSA_daily_20230101_20231130.csv\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "# Define your Region of Interest (AOI) as a GeoJSON-like dictionary.\n",
        "# Example: A small rectangle in an arbitrary location.\n",
        "# REPLACE with your actual coordinates.\n",
        "poligon_AOI = convert_vertices_to_geojson(vertices)\n",
        "region_of_interest = poligon_AOI\n",
        "\n",
        "# Define your date range for time-series data\n",
        "example_start_date = INITIAL_DATE\n",
        "example_end_date = FINAL_DATE\n",
        "\n",
        "# Define the year for land cover analysis\n",
        "example_year_lc = '2020'\n",
        "\n",
        "# Define the desired frequency for time-series aggregation\n",
        "# Options: 'hourly', 'daily', 'monthly', 'yearly'\n",
        "example_frequency = FRECUENCY\n",
        "\n",
        "# --- Ensure output directory exists ---\n",
        "os.makedirs(output_directory, exist_ok=True) # Generic function handles this\n",
        "\n",
        "# Extracting Albedo BSA\n",
        "# print(f\"--- Extracting Albedo BSA ({example_frequency}) ---\")\n",
        "bsa_path = extract_albedo_bsa(\n",
        "    region_geojson=region_of_interest,\n",
        "    start_date_str=example_start_date,\n",
        "    end_date_str=example_end_date,\n",
        "    frequency=example_frequency,\n",
        "    output_dir=output_directory\n",
        ")\n",
        "\n",
        "# Extracting Albedo WSA\n",
        "print(f\"--- Extracting Albedo WSA ({example_frequency}) ---\")\n",
        "wsa_path = extract_albedo_wsa(\n",
        "    region_geojson=region_of_interest,\n",
        "    start_date_str=example_start_date,\n",
        "    end_date_str=example_end_date,\n",
        "    frequency=example_frequency,\n",
        "    output_dir=output_directory\n",
        ")\n",
        "\n",
        "# print(f\"--- Extracting Approximated Solar Radiation ({example_frequency}) ---\")\n",
        "# extract_radiacion_solar(\n",
        "#     region_geojson=region_of_interest,\n",
        "#     start_date_str=example_start_date,\n",
        "#     end_date_str=example_end_date,\n",
        "#     frequency=example_frequency,\n",
        "#     output_dir=output_csv_directory\n",
        "# )\n",
        "\n",
        "# print(f\"--- Extracting Day Temperature ({example_frequency}) ---\")\n",
        "# extract_temperatura_dia(\n",
        "#     region_geojson=region_of_interest,\n",
        "#     start_date_str=example_start_date,\n",
        "#     end_date_str=example_end_date,\n",
        "#     frequency=example_frequency,\n",
        "#     output_dir=output_csv_directory\n",
        "# )\n",
        "\n",
        "# print(f\"--- Extracting Night Temperature ({example_frequency}) ---\")\n",
        "# extract_temperatura_noche(\n",
        "#     region_geojson=region_of_interest,\n",
        "#     start_date_str=example_start_date,\n",
        "#     end_date_str=example_end_date,\n",
        "#     frequency=example_frequency,\n",
        "#     output_dir=output_csv_directory\n",
        "# )\n",
        "\n",
        "# print(f\"--- Extracting Wind Data ({example_frequency}) ---\")\n",
        "# # Note: Wind data from ERA5 is hourly. Generic function will average u/v components\n",
        "# # to the target frequency before calculating speed/direction.\n",
        "# extract_viento(\n",
        "#     region_geojson=region_of_interest,\n",
        "#     start_date_str=example_start_date,\n",
        "#     end_date_str=example_end_date,\n",
        "#     frequency=example_frequency, # e.g., 'daily' will average hourly components to daily means first\n",
        "#     output_dir=output_csv_directory\n",
        "# )\n",
        "\n",
        "# print(f\"--- Extracting Topography Data (Static) ---\")\n",
        "# extract_topography(\n",
        "#     region_geojson=region_of_interest,\n",
        "#     output_dir=output_csv_directory\n",
        "# )\n",
        "\n",
        "# print(f\"--- Extracting Land Cover Data for {example_year_lc} ---\")\n",
        "# extract_land_cover(\n",
        "#     region_geojson=region_of_interest,\n",
        "#     year_str=example_year_lc,\n",
        "#     output_dir=output_csv_directory\n",
        "# )\n",
        "\n"
      ],
      "id": "PKI8nHbD3cAe"
    },
    {
      "cell_type": "code",
      "source": [
        "20/64"
      ],
      "metadata": {
        "id": "98uUOO5QR47J",
        "outputId": "13971927-70db-4b5c-f5d2-541bb023fb99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "98uUOO5QR47J",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3125"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image, PageBreak\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.units import inch\n",
        "from datetime import datetime\n",
        "\n",
        "# 1. Load and preprocess data\n",
        "def load_data(bsa_path, wsa_path):\n",
        "    bsa = pd.read_csv(bsa_path, parse_dates=['timestamp'])\n",
        "    wsa = pd.read_csv(wsa_path, parse_dates=['timestamp'])\n",
        "    # Merge BSA and WSA data\n",
        "    df = pd.merge(bsa, wsa, on='timestamp', suffixes=('_BSA', '_WSA'))\n",
        "    df['month'] = df['timestamp'].dt.month\n",
        "    df['year'] = df['timestamp'].dt.year\n",
        "    return df\n",
        "\n",
        "# 2. Generate statistics tables\n",
        "def calculate_statistics(df):\n",
        "    # Calculate monthly statistics for Albedo BSA\n",
        "    monthly_stats_bsa = df.groupby('month')['Albedo_BSA'].agg([\n",
        "        'mean', 'min', 'max',\n",
        "        lambda x: x.quantile(0.01),  # p1\n",
        "        lambda x: x.quantile(0.05),  # p5\n",
        "        lambda x: x.quantile(0.10),  # p10\n",
        "        lambda x: x.quantile(0.25),  # p25\n",
        "        lambda x: x.quantile(0.50),  # median\n",
        "        lambda x: x.quantile(0.75),  # p75\n",
        "        lambda x: x.quantile(0.90),  # p90\n",
        "        lambda x: x.quantile(0.95),  # p95\n",
        "        lambda x: x.quantile(0.99),  # p99\n",
        "    ])\n",
        "    monthly_stats_bsa.columns = ['average', 'min', 'max', 'p1', 'p5', 'p10', 'p25',\n",
        "                                'p50', 'p75', 'p90', 'p95', 'p99']\n",
        "\n",
        "    # Calculate monthly statistics for Albedo WSA (New)\n",
        "    monthly_stats_wsa = df.groupby('month')['Albedo_WSA'].agg([\n",
        "        'mean', 'min', 'max',\n",
        "        lambda x: x.quantile(0.01),  # p1\n",
        "        lambda x: x.quantile(0.05),  # p5\n",
        "        lambda x: x.quantile(0.10),  # p10\n",
        "        lambda x: x.quantile(0.25),  # p25\n",
        "        lambda x: x.quantile(0.50),  # median\n",
        "        lambda x: x.quantile(0.75),  # p75\n",
        "        lambda x: x.quantile(0.90),  # p90\n",
        "        lambda x: x.quantile(0.95),  # p95\n",
        "        lambda x: x.quantile(0.99),  # p99\n",
        "    ])\n",
        "    monthly_stats_wsa.columns = ['average', 'min', 'max', 'p1', 'p5', 'p10', 'p25',\n",
        "                                'p50', 'p75', 'p90', 'p95', 'p99']\n",
        "\n",
        "    # Return both sets of statistics\n",
        "    return monthly_stats_bsa, monthly_stats_wsa\n",
        "\n",
        "# 3. Create visualizations\n",
        "def create_figures(df, monthly_stats_bsa, monthly_stats_wsa):\n",
        "    # Figure 1: Monthly statistics (BSA)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    monthly_stats_bsa[['average', 'min', 'max']].plot(kind='line', marker='o')\n",
        "    plt.title('Monthly Albedo BSA Statistics') # Updated title\n",
        "    plt.ylabel('Albedo BSA') # Updated label\n",
        "    plt.xlabel('Month')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('monthly_stats_bsa.png') # Updated filename\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 2: Daily values timeline (BSA)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df['timestamp'], df['Albedo_BSA'], label='Daily Albedo BSA') # Updated label\n",
        "    plt.title('Daily Albedo BSA Values') # Updated title\n",
        "    plt.ylabel('Albedo BSA') # Updated label\n",
        "    plt.xlabel('Date')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('daily_values_bsa.png') # Updated filename\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 3: Monthly statistics (WSA) (New)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    monthly_stats_wsa[['average', 'min', 'max']].plot(kind='line', marker='o')\n",
        "    plt.title('Monthly Albedo WSA Statistics') # New title\n",
        "    plt.ylabel('Albedo WSA') # New label\n",
        "    plt.xlabel('Month')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('monthly_stats_wsa.png') # New filename\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 4: Daily values timeline (WSA) (New)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df['timestamp'], df['Albedo_WSA'], label='Daily Albedo WSA') # New label\n",
        "    plt.title('Daily Albedo WSA Values') # New title\n",
        "    plt.ylabel('Albedo WSA') # New label\n",
        "    plt.xlabel('Date')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('daily_values_wsa.png') # New filename\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# 4. Generate PDF report\n",
        "def create_pdf_report(monthly_stats_bsa, monthly_stats_wsa, annual_avg_bsa, annual_avg_wsa, df):\n",
        "\n",
        "    # Define the full path for the PDF using the output_directory variable\n",
        "    pdf_filename = f\"{LOCATION_NAME}_Albedo_Report_{DESCRIPCION}.pdf\"\n",
        "    pdf_path = os.path.join(output_directory, pdf_filename) # Use os.path.join for path safety\n",
        "\n",
        "    # Instantiate SimpleDocTemplate with the full path\n",
        "    doc = SimpleDocTemplate(pdf_path, pagesize=letter)\n",
        "    styles = getSampleStyleSheet()\n",
        "    elements = []\n",
        "\n",
        "    # Add logo if URL is provided\n",
        "    logo_url = 'https://coxenergy.es/img/logos/logo-header.png'\n",
        "    logo_cell = ''\n",
        "    if logo_url:\n",
        "        try:\n",
        "            # ReportLab's Image object can take a URL directly\n",
        "            logo_img = Image(logo_url, width=(64/100)*inch, height=(39/100)*inch, hAlign='LEFT') # Adjust width/height as needed\n",
        "            logo_cell = logo_img\n",
        "            print(f\"Loading logo from URL: {logo_url}\")\n",
        "            elements.append(logo_cell)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load logo from URL {logo_url}: {e}\")\n",
        "            # Optionally add a placeholder or skip the logo\n",
        "\n",
        "\n",
        "    # Title\n",
        "    elements.append(Paragraph(f\"{LOCATION_NAME}\", styles['Title']))\n",
        "    elements.append(Paragraph(f\"Albedo Analysis Report\", styles['Title']))\n",
        "    elements.append(Paragraph(f\"{DESCRIPCION} \", styles['Title']))\n",
        "    elements.append(Spacer(1, 0.1*inch))\n",
        "\n",
        "    # Add map image\n",
        "    # Ensure poligon_AOI is accessible or passed as an argument if needed\n",
        "    try:\n",
        "        coordinates_text = f'{LATITUDE} - {LONGITUDE} \\nPoligon {POLIGON_VERTICES} vertices, \\nradio {RADIO}[Km]'\n",
        "    except NameError:\n",
        "        coordinates_text = \"Coordinates not available\" # Handle case if poligon_AOI is not defined globally\n",
        "\n",
        "    elements.append(Paragraph(\"Study Area Location\", styles['Heading2']))\n",
        "    elements.append(Paragraph(coordinates_text, styles['BodyText']))\n",
        "    elements.append(Spacer(1, 0.1*inch))\n",
        "\n",
        "    # Check if the image file exists before adding it\n",
        "    if os.path.exists('map.png'):\n",
        "        elements.append(Image('map.png', width=6*inch, height=4*inch))\n",
        "    else:\n",
        "        elements.append(Paragraph(\"Map image (map.png) not found.\", styles['BodyText']))\n",
        "\n",
        "    elements.append(Spacer(1, 0.1*inch))\n",
        "\n",
        "\n",
        "    # Methodology\n",
        "    elements.append(Paragraph(\"Methodology\", styles['Heading2'])) # Add a heading for the methodology\n",
        "\n",
        "    # Create separate Paragraph objects for each block of text\n",
        "    methodology_paragraph_1 = \"\"\"\n",
        "    <para align=justify>\n",
        "    The albedo analysis uses MODIS MCD43A3 data (Collection 6.1), including both\n",
        "    black-sky albedo (BSA) and white-sky albedo (WSA) values, aggregated to a daily\n",
        "    frequency. The analysis covers the period from {start_date} to {end_date}.\n",
        "    </para>\n",
        "    \"\"\".format(\n",
        "        start_date=df['timestamp'].min().strftime('%Y-%m-%d'), # Use df to get actual date range\n",
        "        end_date=df['timestamp'].max().strftime('%Y-%m-%d')   # Use df to get actual date range\n",
        "    )\n",
        "    elements.append(Paragraph(methodology_paragraph_1, styles['BodyText']))\n",
        "    elements.append(Spacer(1, 0.25*inch)) # Add a small space between paragraphs\n",
        "\n",
        "    methodology_paragraph_2 = \"\"\"\n",
        "    <para align=justify>\n",
        "    For BSA, values range between {min_val_bsa:.2f} and {max_val_bsa:.2f} with an\n",
        "    overall average of {avg_val_bsa:.2f}. For WSA, values range between {min_val_wsa:.2f}\n",
        "    and {max_val_wsa:.2f} with an overall average of {avg_val_wsa:.2f}.\n",
        "    </para>\n",
        "    \"\"\".format(\n",
        "        min_val_bsa=df['Albedo_BSA'].min(),\n",
        "        max_val_bsa=df['Albedo_BSA'].max(),\n",
        "        avg_val_bsa=annual_avg_bsa,\n",
        "        min_val_wsa=df['Albedo_WSA'].min(),\n",
        "        max_val_wsa=df['Albedo_WSA'].max(),\n",
        "        avg_val_wsa=annual_avg_wsa\n",
        "    )\n",
        "    elements.append(Paragraph(methodology_paragraph_2, styles['BodyText']))\n",
        "    elements.append(PageBreak()) # This line forces a new page after the spacer\n",
        "\n",
        "    # Monthly Statistics Table (BSA)\n",
        "    elements.append(Paragraph(\"Monthly Albedo BSA Statistics\", styles['Heading2'])) # New heading\n",
        "    table_data_bsa = [['Month', 'Avg', 'Min', 'Max', 'P10', 'P50', 'P90']]\n",
        "    for month, data in monthly_stats_bsa.iterrows():\n",
        "        table_data_bsa.append([\n",
        "            str(month),\n",
        "            f\"{data['average']:.2f}\",\n",
        "            f\"{data['min']:.2f}\",\n",
        "            f\"{data['max']:.2f}\",\n",
        "            f\"{data['p10']:.2f}\",\n",
        "            f\"{data['p50']:.2f}\",\n",
        "            f\"{data['p90']:.2f}\"\n",
        "        ])\n",
        "\n",
        "    t_bsa = Table(table_data_bsa)\n",
        "    t_bsa.setStyle(TableStyle([\n",
        "        ('BACKGROUND', (0,0), (-1,0), colors.grey),\n",
        "        ('TEXTCOLOR', (0,0), (-1,0), colors.whitesmoke),\n",
        "        ('ALIGN', (0,0), (-1,-1), 'CENTER'),\n",
        "        ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n",
        "        ('FONTSIZE', (0,0), (-1,0), 12),\n",
        "        ('BOTTOMPADDING', (0,0), (-1,0), 12),\n",
        "        ('BACKGROUND', (0,1), (-1,-1), colors.beige),\n",
        "        ('GRID', (0,0), (-1,-1), 1, colors.black)\n",
        "    ]))\n",
        "    elements.append(t_bsa)\n",
        "    elements.append(Spacer(1, 0.5*inch))\n",
        "\n",
        "    # Add figures (BSA)\n",
        "    elements.append(Paragraph(\"Monthly Albedo BSA Statistics Plot\", styles['Heading2'])) # Updated heading\n",
        "    # Check if image file exists\n",
        "    if os.path.exists('monthly_stats_bsa.png'):\n",
        "        elements.append(Image('monthly_stats_bsa.png', width=6*inch, height=4*inch)) # Updated filename\n",
        "    else:\n",
        "        elements.append(Paragraph(\"Monthly BSA statistics plot (monthly_stats_bsa.png) not found.\", styles['BodyText']))\n",
        "\n",
        "    elements.append(Spacer(1, 0.25*inch))\n",
        "\n",
        "    elements.append(Paragraph(\"Daily Albedo BSA Values Timeline Plot\", styles['Heading2'])) # Updated heading\n",
        "    # Check if image file exists\n",
        "    if os.path.exists('daily_values_bsa.png'):\n",
        "        elements.append(Image('daily_values_bsa.png', width=6*inch, height=4*inch)) # Updated filename\n",
        "    else:\n",
        "        elements.append(Paragraph(\"Daily BSA values timeline plot (daily_values_bsa.png) not found.\", styles['BodyText']))\n",
        "    elements.append(Spacer(1, 1.5*inch)) # Added space before next section\n",
        "    elements.append(PageBreak()) # This line forces a new page after the spacer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Monthly Statistics Table (WSA) (New)\n",
        "    elements.append(Paragraph(\"Monthly Albedo WSA Statistics\", styles['Heading2'])) # New heading\n",
        "    table_data_wsa = [['Month', 'Avg', 'Min', 'Max', 'P10', 'P50', 'P90']]\n",
        "    for month, data in monthly_stats_wsa.iterrows():\n",
        "        table_data_wsa.append([\n",
        "            str(month),\n",
        "            f\"{data['average']:.2f}\",\n",
        "            f\"{data['min']:.2f}\",\n",
        "            f\"{data['max']:.2f}\",\n",
        "            f\"{data['p10']:.2f}\",\n",
        "            f\"{data['p50']:.2f}\",\n",
        "            f\"{data['p90']:.2f}\"\n",
        "        ])\n",
        "\n",
        "    t_wsa = Table(table_data_wsa)\n",
        "    t_wsa.setStyle(TableStyle([\n",
        "        ('BACKGROUND', (0,0), (-1,0), colors.grey),\n",
        "        ('TEXTCOLOR', (0,0), (-1,0), colors.whitesmoke),\n",
        "        ('ALIGN', (0,0), (-1,-1), 'CENTER'),\n",
        "        ('FONTNAME', (0,0), (-1,0), 'Helvetica-Bold'),\n",
        "        ('FONTSIZE', (0,0), (-1,0), 12),\n",
        "        ('BOTTOMPADDING', (0,0), (-1,0), 12),\n",
        "        ('BACKGROUND', (0,1), (-1,-1), colors.beige),\n",
        "        ('GRID', (0,0), (-1,-1), 1, colors.black)\n",
        "    ]))\n",
        "    elements.append(t_wsa)\n",
        "    elements.append(Spacer(1, 0.5*inch))\n",
        "    elements.append(Spacer(1, 0.25*inch))\n",
        "\n",
        "    # Add figures (WSA) (New)\n",
        "    elements.append(Paragraph(\"Monthly Albedo WSA Statistics Plot\", styles['Heading2'])) # New heading\n",
        "    # Check if image file exists\n",
        "    if os.path.exists('monthly_stats_wsa.png'):\n",
        "        elements.append(Image('monthly_stats_wsa.png', width=6*inch, height=4*inch)) # New filename\n",
        "    else:\n",
        "        elements.append(Paragraph(\"Monthly WSA statistics plot (monthly_stats_wsa.png) not found.\", styles['BodyText']))\n",
        "    elements.append(Spacer(1, 0.25*inch))\n",
        "\n",
        "    elements.append(Paragraph(\"Daily Albedo WSA Values Timeline Plot\", styles['Heading2'])) # New heading\n",
        "    # Check if image file exists\n",
        "    if os.path.exists('daily_values_wsa.png'):\n",
        "        elements.append(Image('daily_values_wsa.png', width=6*inch, height=4*inch)) # New filename\n",
        "    else:\n",
        "        elements.append(Paragraph(\"Daily WSA values timeline plot (daily_values_wsa.png) not found.\", styles['BodyText']))\n",
        "    elements.append(Spacer(1, 0.5*inch)) # Added space at the end\n",
        "\n",
        "\n",
        "    doc.build(elements)\n",
        "\n",
        "# Main workflow\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your CSV files\n",
        "    try:\n",
        "      df = load_data(bsa_path, wsa_path)\n",
        "    except Exception as e: # Catch broader exception to see error\n",
        "      print(f\"Initial load failed: {e}. Attempting fallback paths.\")\n",
        "      # Fallback paths - make sure these match the filenames generated by extract_gee_time_series\n",
        "      fallback_bsa_path = '/content/gee_output_data/Albedo_BSA_daily_20000101_20231130.csv'\n",
        "      fallback_wsa_path = '/content/gee_output_data/Albedo_WSA_daily_20000101_20231130.csv'\n",
        "      try:\n",
        "         df = load_data(fallback_bsa_path, fallback_wsa_path)\n",
        "         print(\"Fallback load successful.\")\n",
        "      except Exception as e_fallback:\n",
        "         print(f\"Fallback load failed: {e_fallback}. Cannot proceed with report generation.\")\n",
        "         df = None # Ensure df is None if loading fails\n",
        "         # You might want to exit or handle this failure case appropriately\n",
        "         # For now, the rest of the code will likely raise errors if df is None/empty.\n",
        "\n",
        "\n",
        "    if df is not None and not df.empty: # Only proceed if data was loaded successfully\n",
        "      # Calculate statistics for both BSA and WSA\n",
        "      monthly_stats_bsa, monthly_stats_wsa = calculate_statistics(df) # Now returns two dataframes\n",
        "\n",
        "      # Calculate annual averages for both BSA and WSA\n",
        "      annual_avg_bsa = df['Albedo_BSA'].mean()\n",
        "      annual_avg_wsa = df['Albedo_WSA'].mean() # New\n",
        "\n",
        "      # Generate visualizations for both BSA and WSA\n",
        "      create_figures(df, monthly_stats_bsa, monthly_stats_wsa) # Pass both stats dataframes\n",
        "\n",
        "      # Create PDF report, passing all necessary data\n",
        "      create_pdf_report(monthly_stats_bsa, monthly_stats_wsa, annual_avg_bsa, annual_avg_wsa, df) # Pass all data\n",
        "      print(\"Albedo Report generated successfully.\")\n",
        "\n",
        "    else:\n",
        "      print(\"Skipping report generation due to data loading failure.\")"
      ],
      "metadata": {
        "id": "kHA3qVUSV_92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "0eafddb9-b6ac-4549-d3e1-2149d8df70b7"
      },
      "id": "kHA3qVUSV_92",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading logo from URL: https://coxenergy.es/img/logos/logo-header.png\n",
            "Albedo Report generated successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: descargar  de la session de colab en un zip los archivos {/content/map.html\n",
        "# /content/map.png} y el contenido de la carpeta {/content/gee_output_data}\n",
        "\n",
        "!zip -r content.zip /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcX2QHD2inVr",
        "outputId": "b7435c12-71f1-47b9-c559-18cb26484f80"
      },
      "id": "hcX2QHD2inVr",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/ (stored 0%)\n",
            "updating: content/.config/ (stored 0%)\n",
            "updating: content/.config/active_config (stored 0%)\n",
            "updating: content/.config/.last_opt_in_prompt.yaml (stored 0%)\n",
            "updating: content/.config/config_sentinel (stored 0%)\n",
            "updating: content/.config/.last_survey_prompt.yaml (stored 0%)\n",
            "updating: content/.config/.last_update_check.json (deflated 23%)\n",
            "updating: content/.config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db (deflated 97%)\n",
            "updating: content/.config/logs/ (stored 0%)\n",
            "updating: content/.config/logs/2025.05.14/ (stored 0%)\n",
            "updating: content/.config/logs/2025.05.14/13.38.05.736741.log (deflated 86%)\n",
            "updating: content/.config/logs/2025.05.14/13.37.56.530848.log (deflated 58%)\n",
            "updating: content/.config/logs/2025.05.14/13.38.07.566408.log (deflated 58%)\n",
            "updating: content/.config/logs/2025.05.14/13.38.16.976468.log (deflated 57%)\n",
            "updating: content/.config/logs/2025.05.14/13.38.17.706556.log (deflated 56%)\n",
            "updating: content/.config/logs/2025.05.14/13.37.34.542601.log (deflated 92%)\n",
            "updating: content/.config/logs/2025.05.22/ (stored 0%)\n",
            "updating: content/.config/logs/2025.05.22/06.49.45.761743.log (deflated 57%)\n",
            "updating: content/.config/configurations/ (stored 0%)\n",
            "updating: content/.config/configurations/config_default (stored 0%)\n",
            "updating: content/.config/default_configs.db (deflated 98%)\n",
            "updating: content/map.png (deflated 0%)\n",
            "updating: content/gee_output_data.zip (stored 0%)\n",
            "updating: content/monthly_stats_wsa.png (deflated 5%)\n",
            "updating: content/daily_values_wsa.png (deflated 8%)\n",
            "updating: content/gee_output_data/ (stored 0%)\n",
            "updating: content/gee_output_data/Albedo_BSA_daily_20000101_20231130.csv (deflated 64%)\n",
            "updating: content/gee_output_data/Albedo_WSA_daily_20000101_20231130.csv (deflated 64%)\n",
            "updating: content/gee_output_data/Albedo_BSA_daily_20200101_20231130.csv (deflated 78%)\n",
            "updating: content/gee_output_data/Albedo_WSA_daily_20200101_20231130.csv (deflated 78%)\n",
            "updating: content/monthly_stats_bsa.png (deflated 5%)\n",
            "updating: content/map.html (deflated 66%)\n",
            "updating: content/Albedo_Report.pdf (deflated 20%)\n",
            "updating: content/daily_values_bsa.png (deflated 8%)\n",
            "updating: content/sample_data/ (stored 0%)\n",
            "updating: content/sample_data/anscombe.json (deflated 83%)\n",
            "updating: content/sample_data/README.md (deflated 39%)\n",
            "updating: content/sample_data/california_housing_train.csv (deflated 79%)\n",
            "updating: content/sample_data/california_housing_test.csv (deflated 76%)\n",
            "updating: content/sample_data/mnist_test.csv (deflated 88%)\n",
            "updating: content/sample_data/mnist_train_small.csv (deflated 88%)\n",
            "  adding: content/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/report_files.zip (stored 0%)\n",
            "  adding: content/BELA BELA_map_Zona sin vegetacion/ (stored 0%)\n",
            "  adding: content/BELA BELA_map_Zona sin vegetacion/BELA BELA_map_Zona sin vegetacion.png (deflated 0%)\n",
            "  adding: content/BELA BELA_map_Zona sin vegetacion/Albedo_BSA_daily_20230101_20231130.csv (deflated 78%)\n",
            "  adding: content/BELA BELA_map_Zona sin vegetacion/Albedo_WSA_daily_20230101_20231130.csv (deflated 77%)\n",
            "  adding: content/BELA BELA_map_Zona sin vegetacion/BELA BELA_map_Zona sin vegetacion.html (deflated 66%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}